{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UurCDyuGMCoB",
        "outputId": "feac76a6-33e0-4aba-f689-62e0dead9a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Menginstal library yang dibutuhkan untuk Tahap 3...\n",
            "Instalasi library selesai.\n",
            "Menghubungkan Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive terhubung.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"UAS_Penalaran_Komputer_Tahap_3.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1A_YOUR_NOTEBOOK_ID_HERE\n",
        "\"\"\"\n",
        "\n",
        "# Instalasi Library\n",
        "print(\"Menginstal library yang dibutuhkan untuk Tahap 3...\")\n",
        "# scikit-learn untuk TF-IDF dan SVM/Naive Bayes\n",
        "# transformers untuk BERT\n",
        "# sentence-transformers untuk kemudahan embedding jika menggunakan BERT (opsional)\n",
        "# joblib untuk menyimpan/memuat model dan vektor\n",
        "!pip install pandas scikit-learn transformers sentence-transformers joblib > /dev/null 2>&1\n",
        "print(\"Instalasi library selesai.\")\n",
        "\n",
        "# Koneksi Google Drive\n",
        "print(\"Menghubungkan Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive terhubung.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re # Diperlukan untuk clean_text\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.svm import SVC # Support Vector Machine\n",
        "from sklearn.naive_bayes import MultinomialNB # Naive Bayes\n",
        "from transformers import AutoTokenizer, AutoModel # Untuk BERT\n",
        "from sentence_transformers import SentenceTransformer # Untuk BERT embedding yang lebih mudah\n",
        "import torch # Untuk tensor di PyTorch (digunakan oleh transformers)\n",
        "import json # Untuk menyimpan queries.json\n",
        "import numpy as np # Untuk operasi numerik\n",
        "import scipy.sparse # <<< TAMBAH BARIS INI\n",
        "from typing import Dict, Any, List, Union, Tuple # Untuk type hinting\n",
        "\n",
        "# --- Fungsi untuk membuat jalur (sama seperti Tahap 1 dan 2) ---\n",
        "def create_path(folder_name: str) -> str:\n",
        "    path = os.path.join('/content/drive/MyDrive/', folder_name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        print(f\"Folder '{path}' berhasil dibuat di Google Drive.\")\n",
        "    else:\n",
        "        print(f\"Folder '{path}' sudah ada di Google Drive.\")\n",
        "    return path\n",
        "\n",
        "# --- Fungsi clean_text (DIBAWAH DARI TAHAP 1/2) ---\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Membersihkan teks putusan dari header, footer, disclaimer, dan normalisasi spasi.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'M a h ka m a h A g u n g R e p u blik In d o n esia\\n', '', text)\n",
        "    text = re.sub(r'Disclaimer\\n', '', text)\n",
        "    text = re.sub(r'Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n', '', text)\n",
        "    text = re.sub(r'pelaksanaan fungsi peradilan\\. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu\\.\\n', '', text)\n",
        "    text = re.sub(r'Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n', '', text)\n",
        "    text = re.sub(r'Email : kepaniteraan@mahkamahagung\\.go\\.id\\s+Telp : 021-384 3348 \\(ext\\.318\\)\\n', '', text)\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# --- Fungsi untuk memuat cases.csv ---\n",
        "def load_cases_data(processed_data_folder: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Memuat file cases.csv yang sudah terstruktur dari Tahap 2.\n",
        "    \"\"\"\n",
        "    cases_csv_path = os.path.join(processed_data_folder, 'cases.csv')\n",
        "    print(f\"Mencoba memuat cases.csv dari: {cases_csv_path}\")\n",
        "    if not os.path.exists(cases_csv_path):\n",
        "        print(f\"ERROR: File cases.csv TIDAK DITEMUKAN di '{cases_csv_path}'. Pastikan Tahap 2 sudah selesai dengan benar.\")\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        df = pd.read_csv(cases_csv_path)\n",
        "        if df.empty:\n",
        "            print(f\"Peringatan: File cases.csv ditemukan tetapi KOSONG.\")\n",
        "        else:\n",
        "            print(f\"Berhasil memuat {len(df)} kasus dari cases.csv.\")\n",
        "            for col in ['judul', 'ringkasan_fakta', 'argumen_hukum_utama', 'text_pdf']:\n",
        "                if col in df.columns:\n",
        "                    df[col] = df[col].astype(str).fillna('')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Gagal memuat cases.csv: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "print(\"Fungsi utilitas Tahap 3 berhasil didefinisikan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x3PNLv0MTQ1",
        "outputId": "3c619364-b91a-4658-cb25-ce1cb973a1b2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fungsi utilitas Tahap 3 berhasil didefinisikan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib # Import joblib untuk menyimpan model/vektor\n",
        "\n",
        "# --- Konfigurasi Jalur Proyek ---\n",
        "base_drive_path = '/content/drive/MyDrive/CBR_Data'\n",
        "processed_data_folder = os.path.join(base_drive_path, 'data/processed')\n",
        "eval_data_folder = create_path(os.path.join(base_drive_path, 'data/eval'))\n",
        "# Tambahkan folder untuk menyimpan hasil model/vektor dari Tahap 3\n",
        "models_output_folder = create_path(os.path.join(base_drive_path, 'models'))\n",
        "\n",
        "# --- Muat Data Kasus ---\n",
        "df_cases = load_cases_data(processed_data_folder)\n",
        "\n",
        "# Inisialisasi variabel global yang akan digunakan di retrieve (dan Tahap 4)\n",
        "vectorizer = None # Akan jadi TfidfVectorizer object jika TF-IDF\n",
        "embedding_model = None # Akan jadi SentenceTransformer object jika BERT\n",
        "model_for_retrieval_input = None # Vektor numpy array atau sparse matrix\n",
        "vectorizer_type = None # String: \"TF-IDF\" atau \"BERT_Embedding\"\n",
        "\n",
        "if df_cases.empty:\n",
        "    print(\"Tidak ada data kasus yang dimuat. Proses representasi vektor tidak dapat dilanjutkan.\")\n",
        "else:\n",
        "    documents = df_cases['text_pdf'].tolist()\n",
        "\n",
        "    documents = [doc for doc in documents if doc.strip()]\n",
        "    if not documents:\n",
        "        print(\"ERROR: Tidak ada dokumen teks yang valid untuk dienkode. Pastikan kolom 'text_pdf' terisi.\")\n",
        "    else:\n",
        "        print(f\"Total dokumen teks valid untuk representasi: {len(documents)}\")\n",
        "\n",
        "        # --- PILIH SALAH SATU PENDEKATAN REPRESENTASI VEKTOR ---\n",
        "\n",
        "        # =======================================================\n",
        "        # PENDEKATAN 1: TF-IDF\n",
        "        # =======================================================\n",
        "        # print(\"\\nMemilih pendekatan TF-IDF untuk representasi vektor...\")\n",
        "        # vectorizer = TfidfVectorizer(max_features=5000, stop_words=None)\n",
        "        # case_vectors = vectorizer.fit_transform(documents)\n",
        "        # print(f\"Representasi TF-IDF selesai. Shape: {case_vectors.shape}\")\n",
        "        # vectorizer_type = \"TF-IDF\"\n",
        "        # model_for_retrieval_input = case_vectors # Input untuk model ML\n",
        "\n",
        "        # # --- SIMPAN OUTPUT TF-IDF ---\n",
        "        # try:\n",
        "        #     joblib.dump(vectorizer, os.path.join(models_output_folder, 'tfidf_vectorizer.joblib'))\n",
        "        #     joblib.dump(model_for_retrieval_input, os.path.join(models_output_folder, 'tfidf_case_vectors.joblib'))\n",
        "        #     with open(os.path.join(models_output_folder, 'vectorizer_type.txt'), 'w') as f:\n",
        "        #         f.write(vectorizer_type)\n",
        "        #     print(f\"Output TF-IDF (vectorizer dan vectors) berhasil disimpan ke: {models_output_folder}\")\n",
        "        # except Exception as e:\n",
        "        #     print(f\"ERROR: Gagal menyimpan output TF-IDF: {e}\")\n",
        "\n",
        "\n",
        "        # =======================================================\n",
        "        # PENDEKAN 2: BERT Embedding (Menggunakan Sentence Transformers)\n",
        "        # =======================================================\n",
        "        print(\"\\nMemilih pendekatan BERT Embedding untuk representasi vektor...\")\n",
        "        try:\n",
        "            print(\"Memuat model SentenceTransformer (paraphrase-multilingual-MiniLM-L12-v2)...\")\n",
        "            embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "            batch_size = 32\n",
        "            case_vectors_list = []\n",
        "            for i in range(0, len(documents), batch_size):\n",
        "                batch = documents[i:i+batch_size]\n",
        "                embeddings = embedding_model.encode(batch, convert_to_tensor=True)\n",
        "                case_vectors_list.append(embeddings)\n",
        "                print(f\"  Processed {i+len(batch)}/{len(documents)} documents for embedding.\")\n",
        "\n",
        "            case_vectors = torch.cat(case_vectors_list).cpu().numpy()\n",
        "            print(f\"Representasi BERT Embedding selesai. Shape: {case_vectors.shape}\")\n",
        "            vectorizer_type = \"BERT_Embedding\"\n",
        "            model_for_retrieval_input = case_vectors # Input untuk model ML\n",
        "\n",
        "            # --- SIMPAN OUTPUT BERT EMBEDDING ---\n",
        "            try:\n",
        "                np.save(os.path.join(models_output_folder, 'bert_case_vectors.npy'), model_for_retrieval_input)\n",
        "                with open(os.path.join(models_output_folder, 'vectorizer_type.txt'), 'w') as f:\n",
        "                    f.write(vectorizer_type)\n",
        "                print(f\"Output BERT Embedding (vectors) berhasil disimpan ke: {models_output_folder}\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: Gagal menyimpan output BERT Embedding: {e}\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Gagal saat membuat BERT Embedding: {e}\")\n",
        "            print(\"Pastikan Anda memiliki RAM yang cukup di Colab (cek di bagian atas layar) atau coba model yang lebih kecil.\")\n",
        "            print(\"Anda mungkin perlu merestart runtime dan mencoba TF-IDF sebagai alternatif jika masalah berlanjut.\")\n",
        "            model_for_retrieval_input = None\n",
        "            vectorizer_type = None\n",
        "\n",
        "print(\"Representasi vektor selesai (atau dilewati karena error/data kosong).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5l6LHYKMVL-",
        "outputId": "f0256957-32b3-4806-b9df-5da962c3127c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/drive/MyDrive/CBR_Data/data/eval' sudah ada di Google Drive.\n",
            "Folder '/content/drive/MyDrive/CBR_Data/models' berhasil dibuat di Google Drive.\n",
            "Mencoba memuat cases.csv dari: /content/drive/MyDrive/CBR_Data/data/processed/cases.csv\n",
            "Berhasil memuat 65 kasus dari cases.csv.\n",
            "Total dokumen teks valid untuk representasi: 65\n",
            "\n",
            "Memilih pendekatan BERT Embedding untuk representasi vektor...\n",
            "Memuat model SentenceTransformer (paraphrase-multilingual-MiniLM-L12-v2)...\n",
            "  Processed 32/65 documents for embedding.\n",
            "  Processed 64/65 documents for embedding.\n",
            "  Processed 65/65 documents for embedding.\n",
            "Representasi BERT Embedding selesai. Shape: (65, 384)\n",
            "Output BERT Embedding (vectors) berhasil disimpan ke: /content/drive/MyDrive/CBR_Data/models\n",
            "Representasi vektor selesai (atau dilewati karena error/data kosong).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pastikan df_cases, vectorizer_type, model_for_retrieval_input\n",
        "# sudah didefinisikan dari sel sebelumnya.\n",
        "\n",
        "# Inisialisasi variabel untuk Sel 5 (predict_outcome)\n",
        "X_train, X_test, y_train, y_test = None, None, None, None\n",
        "model_retrieval = None # Model ML (SVM/Naive Bayes) jika dilatih\n",
        "\n",
        "if df_cases.empty or model_for_retrieval_input is None:\n",
        "    print(\"Tidak dapat melanjutkan proses splitting data dan model retrieval karena data kosong atau representasi vektor gagal di Sel 3.\")\n",
        "else:\n",
        "    print(\"\\nMemulai proses Splitting Data dan Model Retrieval...\")\n",
        "\n",
        "    X = model_for_retrieval_input # Ini adalah representasi vektor dari dokumen\n",
        "\n",
        "    valid_text_indices_mask = df_cases['text_pdf'].apply(lambda x: isinstance(x, str) and x.strip() != '')\n",
        "\n",
        "    if valid_text_indices_mask.sum() == 0:\n",
        "        print(\"Peringatan: Tidak ada dokumen teks yang valid untuk splitting. X kosong.\")\n",
        "    else:\n",
        "        y = df_cases.loc[valid_text_indices_mask, 'klasifikasi'].fillna('UNKNOWN').astype(str)\n",
        "\n",
        "        print(f\"Jumlah sampel untuk splitting (X): {X.shape[0]}\")\n",
        "        print(f\"Jumlah sampel untuk label (y): {y.shape[0]}\")\n",
        "        print(f\"Jumlah kelas unik di y: {y.nunique()}\")\n",
        "        print(f\"Distribusi kelas di y:\\n{y.value_counts()}\")\n",
        "\n",
        "        can_stratify = False\n",
        "        if y.nunique() > 1:\n",
        "            class_counts = y.value_counts()\n",
        "            if (class_counts >= 2).all(): # Minimal 2 sampel per kelas untuk stratify dengan test_size=0.2\n",
        "                can_stratify = True\n",
        "                print(\"Kondisi untuk stratify terpenuhi.\")\n",
        "            else:\n",
        "                print(\"Peringatan: Beberapa kelas di 'y' memiliki kurang dari 2 sampel. Stratifikasi mungkin tidak bisa dilakukan atau akan menyebabkan error.\")\n",
        "                print(f\"Kelas dengan kurang dari 2 sampel: {class_counts[class_counts < 2].index.tolist()}\")\n",
        "        else:\n",
        "            print(\"Peringatan: Hanya ada satu kelas unik di 'y'. Stratifikasi tidak dapat dilakukan.\")\n",
        "\n",
        "        if can_stratify:\n",
        "            try:\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "                print(f\"Data berhasil dibagi dengan stratifikasi: Train={len(X_train)} samples, Test={len(X_test)} samples.\")\n",
        "                print(\"Distribusi kelas di y_train:\\n\", y_train.value_counts(normalize=True))\n",
        "                print(\"Distribusi kelas di y_test:\\n\", y_test.value_counts(normalize=True))\n",
        "            except ValueError as ve:\n",
        "                print(f\"ERROR: Gagal splitting dengan stratify=y: {ve}\")\n",
        "                print(\"Ini sering terjadi jika ada kelas dengan sampel sangat sedikit. Mencoba splitting tanpa stratifikasi.\")\n",
        "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "                print(f\"Data berhasil dibagi TANPA stratifikasi: Train={len(X_train)} samples, Test={len(X_test)} samples.\")\n",
        "                print(\"Peringatan: Proporsi kelas mungkin tidak merata di train/test set.\")\n",
        "        else:\n",
        "            print(\"Melakukan splitting tanpa stratifikasi karena kondisi tidak memungkinkan.\")\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "            print(f\"Data berhasil dibagi TANPA stratifikasi: Train={len(X_train)} samples, Test={len(X_test)} samples.\")\n",
        "            print(\"Peringatan: Proporsi kelas mungkin tidak merata di train/test set.\")\n",
        "\n",
        "        # --- Model Retrieval (Klasifikasi/Ranking) ---\n",
        "        if X_train is not None and y_train is not None and len(y_train) > 0:\n",
        "            if vectorizer_type == \"TF-IDF\":\n",
        "                if y.nunique() > 1:\n",
        "                    print(\"Melatih model Multinomial Naive Bayes (MultinomialNB) untuk klasifikasi/retrieval dengan TF-IDF...\")\n",
        "                    try:\n",
        "                        model_retrieval = MultinomialNB()\n",
        "                        model_retrieval.fit(X_train, y_train)\n",
        "                        print(\"Model klasifikasi dilatih.\")\n",
        "                        # --- SIMPAN MODEL KLASIFIKASI ---\n",
        "                        try:\n",
        "                            joblib.dump(model_retrieval, os.path.join(models_output_folder, 'ml_classifier_model.joblib'))\n",
        "                            print(f\"Model klasifikasi berhasil disimpan ke: {models_output_folder}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"ERROR: Gagal menyimpan model klasifikasi: {e}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"ERROR: Gagal melatih MultinomialNB: {e}\")\n",
        "                        model_retrieval = None\n",
        "                else:\n",
        "                    print(\"Tidak cukup kelas unik di 'y' untuk melatih model klasifikasi TF-IDF.\")\n",
        "                    print(\"Retrieval akan dilakukan hanya berdasarkan kemiripan vektor secara langsung (Cosine Similarity).\")\n",
        "\n",
        "            elif vectorizer_type == \"BERT_Embedding\":\n",
        "                print(\"Untuk BERT Embedding, retrieval biasanya dilakukan dengan Cosine Similarity langsung.\")\n",
        "                print(\"Tidak perlu melatih model klasifikasi baru di sini untuk retrieval kemiripan murni.\")\n",
        "                model_retrieval = None\n",
        "\n",
        "            else:\n",
        "                print(\"Tipe vectorizer tidak dikenal.\")\n",
        "                model_retrieval = None\n",
        "        else:\n",
        "            print(\"X_train atau y_train kosong. Tidak ada model retrieval yang dilatih.\")\n",
        "            model_retrieval = None\n",
        "\n",
        "print(\"Splitting data dan setup model retrieval selesai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6bF6FEUMXNr",
        "outputId": "97ca4a75-43b5-4469-8d3e-8efd9a2842cd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Memulai proses Splitting Data dan Model Retrieval...\n",
            "Jumlah sampel untuk splitting (X): 65\n",
            "Jumlah sampel untuk label (y): 65\n",
            "Jumlah kelas unik di y: 6\n",
            "Distribusi kelas di y:\n",
            "klasifikasi\n",
            "Pidana Khusus  Narkotika dan Psikotropika \\n Pidana Khusus  Narkotika dan Psikotropika    51\n",
            "Pidana Khusus  Narkotika dan Psikotropika                                                  6\n",
            "Perdata Agama \\n Perdata Agama  Perceraian                                                 5\n",
            "Pidana Umum  Penghinaan                                                                    1\n",
            "UNKNOWN                                                                                    1\n",
            "Pidana Khusus  Pangan                                                                      1\n",
            "Name: count, dtype: int64\n",
            "Peringatan: Beberapa kelas di 'y' memiliki kurang dari 2 sampel. Stratifikasi mungkin tidak bisa dilakukan atau akan menyebabkan error.\n",
            "Kelas dengan kurang dari 2 sampel: ['Pidana Umum  Penghinaan', 'UNKNOWN', 'Pidana Khusus  Pangan']\n",
            "Melakukan splitting tanpa stratifikasi karena kondisi tidak memungkinkan.\n",
            "Data berhasil dibagi TANPA stratifikasi: Train=52 samples, Test=13 samples.\n",
            "Peringatan: Proporsi kelas mungkin tidak merata di train/test set.\n",
            "Untuk BERT Embedding, retrieval biasanya dilakukan dengan Cosine Similarity langsung.\n",
            "Tidak perlu melatih model klasifikasi baru di sini untuk retrieval kemiripan murni.\n",
            "Splitting data dan setup model retrieval selesai.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "# Pastikan df_cases, vectorizer (jika TF-IDF), embedding_model (jika BERT),\n",
        "# dan model_retrieval (jika ML classifier) sudah didefinisikan dari sel sebelumnya.\n",
        "\n",
        "if df_cases.empty:\n",
        "    print(\"Tidak dapat mendefinisikan fungsi retrieve karena data kasus kosong.\")\n",
        "else:\n",
        "    def retrieve(query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Menemukan top-k kasus lama yang paling mirip dengan query kasus baru.\n",
        "        Args:\n",
        "            query (str): Teks query kasus baru.\n",
        "            k (int): Jumlah kasus teratas yang ingin dikembalikan.\n",
        "        Returns:\n",
        "            List[Dict[str, Any]]: Daftar top-k kasus terurut berdasarkan kemiripan,\n",
        "                                  berisi case_id dan skor kemiripan.\n",
        "        \"\"\"\n",
        "        print(f\"\\nMelakukan retrieval untuk query: '{query[:50]}...' (top-k={k})\")\n",
        "\n",
        "        # 1) Pre-process query (sesuai preprocessing dokumen)\n",
        "        # Untuk kesederhanaan, gunakan pembersihan dasar yang sama dengan text_pdf\n",
        "        processed_query = clean_text(query) # `clean_text` diasumsikan dari Tahap 1/2\n",
        "\n",
        "        # 2) Hitung vektor query\n",
        "        query_vector = None\n",
        "        if vectorizer_type == \"TF-IDF\":\n",
        "            # TF-IDF vectorizer harus sudah di-fit pada corpus dokumen\n",
        "            if 'vectorizer' in locals() and vectorizer is not None:\n",
        "                query_vector = vectorizer.transform([processed_query])\n",
        "            else:\n",
        "                print(\"Error: TF-IDF Vectorizer belum diinisialisasi.\")\n",
        "                return []\n",
        "        elif vectorizer_type == \"BERT_Embedding\":\n",
        "            if 'embedding_model' in locals() and embedding_model is not None:\n",
        "                # Perlu memastikan embedding_model dapat diakses\n",
        "                # Ini mengasumsikan model sudah dimuat di Sel 3\n",
        "                query_vector = embedding_model.encode([processed_query], convert_to_tensor=True).cpu().numpy()\n",
        "            else:\n",
        "                print(\"Error: BERT Embedding Model belum diinisialisasi.\")\n",
        "                return []\n",
        "        else:\n",
        "            print(\"Error: Tipe vectorizer tidak dikenal.\")\n",
        "            return []\n",
        "\n",
        "        if query_vector is None:\n",
        "            return []\n",
        "\n",
        "        results = []\n",
        "        if vectorizer_type == \"TF-IDF\" and model_retrieval is not None:\n",
        "            # Jika menggunakan model klasifikasi (SVM/Naive Bayes)\n",
        "            print(\"Menggunakan model klasifikasi untuk memprediksi kategori query dan mencari kasus serupa di kategori itu.\")\n",
        "            # Ini adalah pendekatan klasifikasi, bukan retrieval kemiripan langsung di seluruh korpus\n",
        "            # Untuk retrieval murni, kita biasanya mencari kemiripan vektor langsung.\n",
        "            # Jika model_retrieval adalah classifier, maka ini akan memprediksi kelas\n",
        "            # dan kita perlu mengambil kasus dari kelas yang diprediksi.\n",
        "\n",
        "            # Prediksi probabilitas kelas untuk query\n",
        "            if hasattr(model_retrieval, 'predict_proba'):\n",
        "                proba = model_retrieval.predict_proba(query_vector)\n",
        "                predicted_class_idx = np.argmax(proba)\n",
        "                predicted_class = model_retrieval.classes_[predicted_class_idx]\n",
        "                print(f\"Query diprediksi masuk kategori: '{predicted_class}'\")\n",
        "\n",
        "                # Sekarang cari kasus yang paling mirip di antara kasus yang berlabel predicted_class\n",
        "                # Ini memerlukan pemetaan kembali embeddings/TF-IDF ke dokumen asli dan labelnya.\n",
        "                # Untuk kesederhanaan demo, kita akan kembali ke cosine similarity di seluruh korpus.\n",
        "                # Implementasi klasifikasi untuk retrieval lebih kompleks dan butuh label jelas.\n",
        "\n",
        "                # Fallback ke cosine similarity untuk retrieval kemiripan langsung\n",
        "                similarities = cosine_similarity(query_vector, model_for_retrieval_input).flatten()\n",
        "            else:\n",
        "                print(\"Model retrieval tidak memiliki predict_proba. Melakukan retrieval berdasarkan cosine similarity langsung.\")\n",
        "                similarities = cosine_similarity(query_vector, model_for_retrieval_input).flatten()\n",
        "\n",
        "        elif vectorizer_type == \"BERT_Embedding\" or (vectorizer_type == \"TF-IDF\" and model_retrieval is None):\n",
        "            # 3) Hitung cosine-similarity dengan semua case vectors\n",
        "            # Mengasumsikan model_for_retrieval_input sudah berisi embeddings/TF-IDF dari semua kasus\n",
        "            similarities = cosine_similarity(query_vector, model_for_retrieval_input).flatten()\n",
        "\n",
        "        else:\n",
        "            print(\"Error: Konfigurasi model retrieval tidak didukung.\")\n",
        "            return []\n",
        "\n",
        "        # Mendapatkan indeks kasus terurut berdasarkan kemiripan\n",
        "        # Menggunakan argpartition untuk efisiensi jika k kecil\n",
        "        top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
        "        # Urutkan berdasarkan kemiripan (descending)\n",
        "        sorted_top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])][::-1]\n",
        "\n",
        "        # 4) Kembalikan top-k case id (dan informasi relevan lainnya)\n",
        "        # Pastikan indeks ini merujuk ke DataFrame asli\n",
        "        # Karena `documents` dan `model_for_retrieval_input` mungkin tidak memiliki indeks yang sama\n",
        "        # dengan df_cases akibat pembersihan dokumen kosong, kita perlu memetakan kembali.\n",
        "\n",
        "        # Dapatkan indeks asli df_cases yang tidak dibuang karena text_pdf kosong\n",
        "        original_valid_indices = df_cases[df_cases['text_pdf'].apply(lambda x: isinstance(x, str) and x.strip() != '')].index.tolist()\n",
        "\n",
        "        # Petakan indeks dari `model_for_retrieval_input` kembali ke indeks DataFrame asli\n",
        "        mapped_indices = [original_valid_indices[idx] for idx in sorted_top_k_indices]\n",
        "\n",
        "        for idx in mapped_indices:\n",
        "            case_info = df_cases.loc[idx]\n",
        "            similarity_score = similarities[original_valid_indices.index(idx)] # Dapatkan skor kemiripan yang benar\n",
        "            results.append({\n",
        "                \"case_id\": case_info['nomor'], # Menggunakan nomor perkara sebagai case_id\n",
        "                \"judul\": case_info['judul'],\n",
        "                \"klasifikasi\": case_info['klasifikasi'],\n",
        "                \"similarity_score\": float(similarity_score), # Pastikan float untuk JSON\n",
        "                \"link\": case_info['link']\n",
        "            })\n",
        "\n",
        "        print(f\"Retrieval selesai. Ditemukan {len(results)} kasus.\")\n",
        "        return results\n",
        "\n",
        "print(\"Fungsi retrieval berhasil didefinisikan (atau dilewati).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz3eVD17MZEz",
        "outputId": "f9461159-3f15-4890-cb95-f4efc6501541"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fungsi retrieval berhasil didefinisikan (atau dilewati).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "# Pastikan df_cases, vectorizer, embedding_model, model_for_retrieval_input,\n",
        "# dan vectorizer_type sudah didefinisikan dari sel sebelumnya.\n",
        "# Pastikan juga fungsi clean_text sudah ada (dari Sel 2).\n",
        "\n",
        "if df_cases.empty:\n",
        "    print(\"Tidak dapat mendefinisikan fungsi retrieve karena data kasus kosong.\")\n",
        "else:\n",
        "    def retrieve(query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Menemukan top-k kasus lama yang paling mirip dengan query kasus baru.\n",
        "        Args:\n",
        "            query (str): Teks query kasus baru.\n",
        "            k (int): Jumlah kasus teratas yang ingin dikembalikan.\n",
        "        Returns:\n",
        "            List[Dict[str, Any]]: Daftar top-k kasus terurut berdasarkan kemiripan,\n",
        "                                  berisi case_id dan skor kemiripan.\n",
        "        \"\"\"\n",
        "        print(f\"\\nMelakukan retrieval untuk query: '{query[:50]}...' (top-k={k})\")\n",
        "\n",
        "        # 1) Pre-process query\n",
        "        if 'clean_text' not in globals() or not callable(clean_text):\n",
        "            print(\"ERROR: Fungsi 'clean_text' tidak ditemukan. Pastikan Sel 2 sudah dijalankan.\")\n",
        "            return []\n",
        "\n",
        "        processed_query = clean_text(query)\n",
        "        print(f\"DEBUG: Query setelah pre-processing: '{processed_query[:50]}...'\")\n",
        "\n",
        "        # 2) Hitung vektor query\n",
        "        query_vector = None\n",
        "        if vectorizer_type == \"TF-IDF\":\n",
        "            if vectorizer is not None:\n",
        "                try:\n",
        "                    query_vector = vectorizer.transform([processed_query])\n",
        "                    print(f\"DEBUG: Query vector TF-IDF shape: {query_vector.shape}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Gagal transformasi query TF-IDF: {e}\")\n",
        "                    return []\n",
        "            else:\n",
        "                print(\"ERROR: TF-IDF Vectorizer belum diinisialisasi di Sel 3.\")\n",
        "                return []\n",
        "        elif vectorizer_type == \"BERT_Embedding\":\n",
        "            if embedding_model is not None:\n",
        "                try:\n",
        "                    query_vector = embedding_model.encode([processed_query], convert_to_tensor=True).cpu().numpy()\n",
        "                    print(f\"DEBUG: Query vector BERT Embedding shape: {query_vector.shape}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Gagal membuat embedding query BERT: {e}\")\n",
        "                    return []\n",
        "            else:\n",
        "                print(\"ERROR: BERT Embedding Model belum diinisialisasi di Sel 3.\")\n",
        "                return []\n",
        "        else:\n",
        "            print(\"ERROR: Tipe vectorizer tidak dikenal atau belum diatur di Sel 3.\")\n",
        "            return []\n",
        "\n",
        "        if query_vector is None or model_for_retrieval_input is None:\n",
        "            print(\"ERROR: Query vector atau model_for_retrieval_input kosong. Tidak bisa menghitung kemiripan.\")\n",
        "            return []\n",
        "\n",
        "        # Pastikan model_for_retrieval_input memiliki bentuk yang benar untuk cosine_similarity\n",
        "        if not isinstance(model_for_retrieval_input, (np.ndarray, scipy.sparse.csr.csr_matrix, torch.Tensor)): # Tambahkan scipy.sparse jika pakai TF-IDF\n",
        "             print(f\"ERROR: model_for_retrieval_input memiliki tipe yang tidak didukung untuk similarity: {type(model_for_retrieval_input)}\")\n",
        "             return []\n",
        "\n",
        "        # 3) Hitung cosine-similarity dengan semua case vectors\n",
        "        try:\n",
        "            similarities = cosine_similarity(query_vector, model_for_retrieval_input).flatten()\n",
        "            print(f\"DEBUG: Similarities shape: {similarities.shape}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Gagal menghitung cosine similarity: {e}\")\n",
        "            print(f\"  Query vector shape: {query_vector.shape if query_vector is not None else 'None'}\")\n",
        "            print(f\"  Corpus vectors shape: {model_for_retrieval_input.shape if model_for_retrieval_input is not None else 'None'}\")\n",
        "            return []\n",
        "\n",
        "        # Mendapatkan indeks kasus terurut berdasarkan kemiripan\n",
        "        if len(similarities) < k:\n",
        "            print(f\"Peringatan: Jumlah sampel ({len(similarities)}) kurang dari k ({k}). Mengembalikan semua sampel.\")\n",
        "            top_k_indices = np.arange(len(similarities))\n",
        "        else:\n",
        "            top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
        "\n",
        "        # Urutkan berdasarkan kemiripan (descending)\n",
        "        sorted_top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])][::-1]\n",
        "\n",
        "        # 4) Kembalikan top-k case id (dan informasi relevan lainnya)\n",
        "        results = []\n",
        "        # Dapatkan indeks asli df_cases yang tidak dibuang karena text_pdf kosong\n",
        "        original_valid_indices = df_cases[df_cases['text_pdf'].apply(lambda x: isinstance(x, str) and x.strip() != '')].index.tolist()\n",
        "\n",
        "        # Pastikan original_valid_indices memiliki panjang yang sama dengan model_for_retrieval_input.shape[0]\n",
        "        if len(original_valid_indices) != model_for_retrieval_input.shape[0]:\n",
        "            print(f\"ERROR: Ketidakcocokan panjang antara original_valid_indices ({len(original_valid_indices)}) dan model_for_retrieval_input ({model_for_retrieval_input.shape[0]}).\")\n",
        "            print(\"Ini bisa terjadi jika ada dokumen yang dibuang setelah embedding atau kesalahan indexing.\")\n",
        "            return []\n",
        "\n",
        "        for idx_in_corpus in sorted_top_k_indices:\n",
        "            original_df_index = original_valid_indices[idx_in_corpus]\n",
        "            case_info = df_cases.loc[original_df_index]\n",
        "            similarity_score = similarities[idx_in_corpus] # Ambil skor dari array similarities\n",
        "            results.append({\n",
        "                \"case_id\": case_info['nomor'],\n",
        "                \"judul\": case_info['judul'],\n",
        "                \"klasifikasi\": case_info['klasifikasi'],\n",
        "                \"similarity_score\": float(similarity_score),\n",
        "                \"link\": case_info['link']\n",
        "            })\n",
        "\n",
        "        print(f\"Retrieval selesai. Ditemukan {len(results)} kasus.\")\n",
        "        return results\n",
        "\n",
        "print(\"Fungsi retrieval berhasil didefinisikan (atau dilewati).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8imB8Z2MayP",
        "outputId": "95a6b0de-9666-4075-d1df-661d56a70949"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fungsi retrieval berhasil didefinisikan (atau dilewati).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Siapkan 5-10 query uji beserta ground-truth case_id (jika ada) ---\n",
        "sample_queries = [\n",
        "    {\n",
        "        \"query_id\": \"Q1_Narkotika_1\",\n",
        "        \"text\": \"Seorang terdakwa ditangkap karena memiliki 5 gram sabu-sabu dan dijerat pasal 112 Undang-Undang Narkotika.\",\n",
        "        \"ground_truth_case_id\": [] # Ganti dengan nomor perkara yang benar dari data Anda jika ada\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q2_Narkotika_2\",\n",
        "        \"text\": \"Kasus peredaran gelap narkotika jenis ganja yang melibatkan lebih dari 1 kilogram.\",\n",
        "        \"ground_truth_case_id\": []\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q3_Perdata_Wanprestasi_1\",\n",
        "        \"text\": \"Gugatan wanprestasi karena salah satu pihak tidak memenuhi perjanjian jual beli tanah yang sudah disepakati.\",\n",
        "        \"ground_truth_case_id\": []\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q4_Perdata_Perceraian_1\",\n",
        "        \"text\": \"Permohonan cerai gugat di Pengadilan Agama karena perselisihan dan pertengkaran terus-menerus.\",\n",
        "        \"ground_truth_case_id\": []\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"Q5_Pidana_Pencurian_1\",\n",
        "        \"text\": \"Kasus pencurian sepeda motor yang dilakukan oleh anak di bawah umur di wilayah Jakarta.\",\n",
        "        \"ground_truth_case_id\": []\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Lakukan pengujian awal retrieval ---\n",
        "print(\"\\n--- Melakukan Pengujian Awal Fungsi Retrieval ---\")\n",
        "retrieval_test_results = []\n",
        "# Pastikan fungsi retrieve didefinisikan dan model_for_retrieval_input tidak kosong\n",
        "if 'retrieve' in locals() and callable(retrieve) and model_for_retrieval_input is not None and model_for_retrieval_input.shape[0] > 0:\n",
        "    for query_data in sample_queries:\n",
        "        query_text = query_data['text']\n",
        "        query_id = query_data['query_id']\n",
        "        top_k_results = retrieve(query_text, k=5)\n",
        "        print(f\"\\nQuery ID: {query_id}\")\n",
        "        print(f\"Top 5 Kasus Mirip:\")\n",
        "        if top_k_results:\n",
        "            for res in top_k_results:\n",
        "                print(f\"  - Case ID: {res['case_id']}, Judul: {res['judul'][:70]}..., Skor: {res['similarity_score']:.4f}\")\n",
        "        else:\n",
        "            print(\"  Tidak ada hasil retrieval.\")\n",
        "\n",
        "        retrieval_test_results.append({\n",
        "            \"query_id\": query_id,\n",
        "            \"query_text\": query_text,\n",
        "            \"ground_truth_case_id\": query_data['ground_truth_case_id'],\n",
        "            \"retrieved_cases\": top_k_results\n",
        "        })\n",
        "else:\n",
        "    print(\"Fungsi 'retrieve' tidak dapat ditemukan, atau model data kosong. Pastikan semua sel sebelumnya berhasil dijalankan dan data tersedia.\")\n",
        "\n",
        "\n",
        "# --- Simpan query uji ke /data/eval/queries.json ---\n",
        "queries_json_path = os.path.join(eval_data_folder, 'queries.json')\n",
        "try:\n",
        "    with open(queries_json_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(sample_queries, f, indent=4, ensure_ascii=False)\n",
        "    print(f\"\\nQuery uji berhasil disimpan ke: {queries_json_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Gagal menyimpan queries.json: {e}\")\n",
        "\n",
        "print(\"\\nPROSES TAHAP 3 (CASE RETRIEVAL) SELESAI.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf0YQUFOOVdi",
        "outputId": "288fbed0-57e7-4f3b-8e85-9e6d1eb3ac8c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Melakukan Pengujian Awal Fungsi Retrieval ---\n",
            "\n",
            "Melakukan retrieval untuk query: 'Seorang terdakwa ditangkap karena memiliki 5 gram ...' (top-k=5)\n",
            "DEBUG: Query setelah pre-processing: 'Seorang terdakwa ditangkap karena memiliki 5 gram ...'\n",
            "DEBUG: Query vector BERT Embedding shape: (1, 384)\n",
            "DEBUG: Similarities shape: (65,)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "\n",
            "Query ID: Q1_Narkotika_1\n",
            "Top 5 Kasus Mirip:\n",
            "  - Case ID: 83/Pid.Sus/2015/PN. Nnk, Judul: Putusan PN NUNUKAN Nomor 83/Pid.Sus/2015/PN. Nnk Tanggal 6 Juli 2105 —..., Skor: 0.2941\n",
            "  - Case ID: 326/PID.SUS/2025/PT PBR, Judul: Putusan PT PEKANBARU Nomor 326/PID.SUS/2025/PT PBR Tanggal 26 Juni 202..., Skor: 0.2934\n",
            "  - Case ID: 326/PID.SUS/2025/PT PBR, Judul: Putusan PT PEKANBARU Nomor 326/PID.SUS/2025/PT PBR Tanggal 26 Juni 202..., Skor: 0.2934\n",
            "  - Case ID: 5492 K/Pid.Sus/2024, Judul: Putusan MAHKAMAH AGUNG Nomor 5492 K/Pid.Sus/2024 Tanggal 19 September ..., Skor: 0.2928\n",
            "  - Case ID: 63/Pid.Sus/2025/PN Ksp, Judul: Putusan PN KUALA SIMPANG Nomor 63/Pid.Sus/2025/PN Ksp Tanggal 26 Juni ..., Skor: 0.2797\n",
            "\n",
            "Melakukan retrieval untuk query: 'Kasus peredaran gelap narkotika jenis ganja yang m...' (top-k=5)\n",
            "DEBUG: Query setelah pre-processing: 'Kasus peredaran gelap narkotika jenis ganja yang m...'\n",
            "DEBUG: Query vector BERT Embedding shape: (1, 384)\n",
            "DEBUG: Similarities shape: (65,)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "\n",
            "Query ID: Q2_Narkotika_2\n",
            "Top 5 Kasus Mirip:\n",
            "  - Case ID: 83/Pid.Sus/2015/PN. Nnk, Judul: Putusan PN NUNUKAN Nomor 83/Pid.Sus/2015/PN. Nnk Tanggal 6 Juli 2105 —..., Skor: 0.2415\n",
            "  - Case ID: 5492 K/Pid.Sus/2024, Judul: Putusan MAHKAMAH AGUNG Nomor 5492 K/Pid.Sus/2024 Tanggal 19 September ..., Skor: 0.2402\n",
            "  - Case ID: 137/Pid.Sus/2025/PN Wtp, Judul: Putusan PN WATAMPONE Nomor 137/Pid.Sus/2025/PN Wtp Tanggal 26 Juni 202..., Skor: 0.2290\n",
            "  - Case ID: 63/Pid.Sus/2025/PN Ksp, Judul: Putusan PN KUALA SIMPANG Nomor 63/Pid.Sus/2025/PN Ksp Tanggal 26 Juni ..., Skor: 0.2284\n",
            "  - Case ID: 484/PID.SUS/2025/PT SMG, Judul: Putusan PT SEMARANG Nomor 484/PID.SUS/2025/PT SMG Tanggal 26 Juni 2025..., Skor: 0.2280\n",
            "\n",
            "Melakukan retrieval untuk query: 'Gugatan wanprestasi karena salah satu pihak tidak ...' (top-k=5)\n",
            "DEBUG: Query setelah pre-processing: 'Gugatan wanprestasi karena salah satu pihak tidak ...'\n",
            "DEBUG: Query vector BERT Embedding shape: (1, 384)\n",
            "DEBUG: Similarities shape: (65,)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "\n",
            "Query ID: Q3_Perdata_Wanprestasi_1\n",
            "Top 5 Kasus Mirip:\n",
            "  - Case ID: 270/PID.SUS/2025/PT PDG, Judul: Putusan PT PADANG Nomor 270/PID.SUS/2025/PT PDG Tanggal 26 Juni 2025 —..., Skor: 0.2513\n",
            "  - Case ID: 263/PID.SUS/2025/PT PDG, Judul: Putusan PT PADANG Nomor 263/PID.SUS/2025/PT PDG Tanggal 26 Juni 2025 —..., Skor: 0.2513\n",
            "  - Case ID: 935/PID.SUS/2025/PT SBY, Judul: Putusan PT SURABAYA Nomor 935/PID.SUS/2025/PT SBY Tanggal 26 Juni 2025..., Skor: 0.2512\n",
            "  - Case ID: 46/PID.SUS/2025/PT TJS, Judul: Putusan PT KALIMANTAN UTARA Nomor 46/PID.SUS/2025/PT TJS Tanggal 26 Ju..., Skor: 0.2486\n",
            "  - Case ID: 304/PID.SUS/2025/PT PBR, Judul: Putusan PT PEKANBARU Nomor 304/PID.SUS/2025/PT PBR Tanggal 26 Juni 202..., Skor: 0.2448\n",
            "\n",
            "Melakukan retrieval untuk query: 'Permohonan cerai gugat di Pengadilan Agama karena ...' (top-k=5)\n",
            "DEBUG: Query setelah pre-processing: 'Permohonan cerai gugat di Pengadilan Agama karena ...'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-21-4261560080.py:63: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  if not isinstance(model_for_retrieval_input, (np.ndarray, scipy.sparse.csr.csr_matrix, torch.Tensor)): # Tambahkan scipy.sparse jika pakai TF-IDF\n",
            "/tmp/ipython-input-21-4261560080.py:63: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  if not isinstance(model_for_retrieval_input, (np.ndarray, scipy.sparse.csr.csr_matrix, torch.Tensor)): # Tambahkan scipy.sparse jika pakai TF-IDF\n",
            "/tmp/ipython-input-21-4261560080.py:63: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  if not isinstance(model_for_retrieval_input, (np.ndarray, scipy.sparse.csr.csr_matrix, torch.Tensor)): # Tambahkan scipy.sparse jika pakai TF-IDF\n",
            "/tmp/ipython-input-21-4261560080.py:63: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  if not isinstance(model_for_retrieval_input, (np.ndarray, scipy.sparse.csr.csr_matrix, torch.Tensor)): # Tambahkan scipy.sparse jika pakai TF-IDF\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Query vector BERT Embedding shape: (1, 384)\n",
            "DEBUG: Similarities shape: (65,)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "\n",
            "Query ID: Q4_Perdata_Perceraian_1\n",
            "Top 5 Kasus Mirip:\n",
            "  - Case ID: 1489/Pdt.G/2025/PA.Pwd, Judul: Putusan PA PURWODADI Nomor 1489/Pdt.G/2025/PA.Pwd Tanggal 19 Juni 2025..., Skor: 0.4230\n",
            "  - Case ID: 154/PID.SUS/2025/PT MAM, Judul: Putusan PT SULAWESI BARAT Nomor 154/PID.SUS/2025/PT MAM Tanggal 26 Jun..., Skor: 0.3536\n",
            "  - Case ID: 1445/Pdt.G/2025/PA.Pwd, Judul: Putusan PA PURWODADI Nomor 1445/Pdt.G/2025/PA.Pwd Tanggal 19 Juni 2025..., Skor: 0.3398\n",
            "  - Case ID: 1445/Pdt.G/2025/PA.Pwd, Judul: Putusan PA PURWODADI Nomor 1445/Pdt.G/2025/PA.Pwd Tanggal 19 Juni 2025..., Skor: 0.3398\n",
            "  - Case ID: 106/Pid.Sus/2025/PN Pkb, Judul: Putusan PN Pangkalan Balai Nomor 106/Pid.Sus/2025/PN Pkb Tanggal 26 Ju..., Skor: 0.3387\n",
            "\n",
            "Melakukan retrieval untuk query: 'Kasus pencurian sepeda motor yang dilakukan oleh a...' (top-k=5)\n",
            "DEBUG: Query setelah pre-processing: 'Kasus pencurian sepeda motor yang dilakukan oleh a...'\n",
            "DEBUG: Query vector BERT Embedding shape: (1, 384)\n",
            "DEBUG: Similarities shape: (65,)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "\n",
            "Query ID: Q5_Pidana_Pencurian_1\n",
            "Top 5 Kasus Mirip:\n",
            "  - Case ID: 5492 K/Pid.Sus/2024, Judul: Putusan MAHKAMAH AGUNG Nomor 5492 K/Pid.Sus/2024 Tanggal 19 September ..., Skor: 0.4128\n",
            "  - Case ID: 83/Pid.Sus/2015/PN. Nnk, Judul: Putusan PN NUNUKAN Nomor 83/Pid.Sus/2015/PN. Nnk Tanggal 6 Juli 2105 —..., Skor: 0.4065\n",
            "  - Case ID: 63/Pid.Sus/2025/PN Ksp, Judul: Putusan PN KUALA SIMPANG Nomor 63/Pid.Sus/2025/PN Ksp Tanggal 26 Juni ..., Skor: 0.3983\n",
            "  - Case ID: 137/Pid.Sus/2025/PN Wtp, Judul: Putusan PN WATAMPONE Nomor 137/Pid.Sus/2025/PN Wtp Tanggal 26 Juni 202..., Skor: 0.3909\n",
            "  - Case ID: 941/PID.SUS/2025/PT SBY, Judul: Putusan PT SURABAYA Nomor 941/PID.SUS/2025/PT SBY Tanggal 26 Juni 2025..., Skor: 0.3842\n",
            "\n",
            "Query uji berhasil disimpan ke: /content/drive/MyDrive/CBR_Data/data/eval/queries.json\n",
            "\n",
            "PROSES TAHAP 3 (CASE RETRIEVAL) SELESAI.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-21-4261560080.py:63: DeprecationWarning: Please import `csr_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.csr` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  if not isinstance(model_for_retrieval_input, (np.ndarray, scipy.sparse.csr.csr_matrix, torch.Tensor)): # Tambahkan scipy.sparse jika pakai TF-IDF\n"
          ]
        }
      ]
    }
  ]
}