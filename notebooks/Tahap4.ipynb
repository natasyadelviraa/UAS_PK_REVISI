{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfXhrbYAPWkd",
        "outputId": "2ec94ffb-de1e-432e-94a5-73794d806080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Menginstal library yang dibutuhkan untuk Tahap 4...\n",
            "Instalasi library selesai.\n",
            "Menghubungkan Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive terhubung.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"UAS_Penalaran_Komputer_Tahap_4.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1B_YOUR_NOTEBOOK_ID_HERE\n",
        "\"\"\"\n",
        "\n",
        "# Instalasi Library\n",
        "print(\"Menginstal library yang dibutuhkan untuk Tahap 4...\")\n",
        "!pip install pandas joblib transformers sentence-transformers numpy > /dev/null 2>&1\n",
        "print(\"Instalasi library selesai.\")\n",
        "\n",
        "# Koneksi Google Drive\n",
        "print(\"Menghubungkan Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive terhubung.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import joblib # Untuk memuat model TF-IDF\n",
        "import numpy as np # Untuk memuat BERT embeddings\n",
        "from sentence_transformers import SentenceTransformer # Untuk model embedding BERT\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Untuk vectorizer TF-IDF\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re # Untuk clean_text\n",
        "from typing import List, Dict, Any, Union, Tuple\n",
        "\n",
        "# --- Fungsi untuk membuat jalur (diulang agar tersedia di Tahap 4) ---\n",
        "def create_path(folder_name: str) -> str:\n",
        "    path = os.path.join('/content/drive/MyDrive/', folder_name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        print(f\"Folder '{path}' berhasil dibuat di Google Drive.\")\n",
        "    else:\n",
        "        print(f\"Folder '{path}' sudah ada di Google Drive.\")\n",
        "    return path\n",
        "\n",
        "# --- Fungsi clean_text (diulang agar tersedia di Tahap 4) ---\n",
        "def clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'M a h ka m a h A g u n g R e p u blik In d o n esia\\n', '', text)\n",
        "    text = re.sub(r'Disclaimer\\n', '', text)\n",
        "    text = re.sub(r'Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n', '', text)\n",
        "    text = re.sub(r'pelaksanaan fungsi peradilan\\. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu\\.\\n', '', text)\n",
        "    text = re.sub(r'Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n', '', text)\n",
        "    text = re.sub(r'Email : kepaniteraan@mahkamahagung\\.go\\.id\\s+Telp : 021-384 3348 \\(ext\\.318\\)\\n', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# --- Fungsi untuk memuat cases.csv (diulang agar tersedia di Tahap 4) ---\n",
        "def load_cases_data(processed_data_folder: str) -> pd.DataFrame:\n",
        "    cases_csv_path = os.path.join(processed_data_folder, 'cases.csv')\n",
        "    print(f\"Mencoba memuat cases.csv dari: {cases_csv_path}\")\n",
        "    if not os.path.exists(cases_csv_path):\n",
        "        print(f\"ERROR: File cases.csv TIDAK DITEMUKAN di '{cases_csv_path}'. Pastikan Tahap 2 sudah selesai dengan benar.\")\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        df = pd.read_csv(cases_csv_path)\n",
        "        if df.empty:\n",
        "            print(f\"Peringatan: File cases.csv ditemukan tetapi KOSONG.\")\n",
        "        else:\n",
        "            print(f\"Berhasil memuat {len(df)} kasus dari cases.csv.\")\n",
        "            for col in ['judul', 'ringkasan_fakta', 'argumen_hukum_utama', 'text_pdf', 'amar', 'amar_lainnya', 'catatan_amar']:\n",
        "                if col in df.columns:\n",
        "                    df[col] = df[col].astype(str).fillna('')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Gagal memuat cases.csv: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# --- Load Model dan Vektor dari Tahap 3 ---\n",
        "# Konfigurasi Jalur\n",
        "base_drive_path = '/content/drive/MyDrive/CBR_Data'\n",
        "models_output_folder = os.path.join(base_drive_path, 'models')\n",
        "processed_data_folder = os.path.join(base_drive_path, 'data/processed') # Untuk load cases.csv\n",
        "\n",
        "# Inisialisasi variabel untuk Tahap 4\n",
        "loaded_vectorizer = None # TfidfVectorizer jika TF-IDF\n",
        "loaded_embedding_model = None # SentenceTransformer jika BERT\n",
        "loaded_case_vectors = None # Numpy array atau sparse matrix dari kasus\n",
        "loaded_vectorizer_type = None # \"TF-IDF\" atau \"BERT_Embedding\"\n",
        "\n",
        "print(\"Memuat model dan vektor dari Tahap 3...\")\n",
        "try:\n",
        "    with open(os.path.join(models_output_folder, 'vectorizer_type.txt'), 'r') as f:\n",
        "        loaded_vectorizer_type = f.read().strip()\n",
        "    print(f\"Tipe vectorizer yang digunakan: {loaded_vectorizer_type}\")\n",
        "\n",
        "    if loaded_vectorizer_type == \"TF-IDF\":\n",
        "        loaded_vectorizer = joblib.load(os.path.join(models_output_folder, 'tfidf_vectorizer.joblib'))\n",
        "        loaded_case_vectors = joblib.load(os.path.join(models_output_folder, 'tfidf_case_vectors.joblib'))\n",
        "        print(\"TF-IDF vectorizer dan case vectors berhasil dimuat.\")\n",
        "    elif loaded_vectorizer_type == \"BERT_Embedding\":\n",
        "        print(\"Memuat ulang SentenceTransformer model untuk BERT Embedding...\")\n",
        "        loaded_embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        loaded_case_vectors = np.load(os.path.join(models_output_folder, 'bert_case_vectors.npy'))\n",
        "        print(\"BERT embedding model dan case vectors berhasil dimuat.\")\n",
        "    else:\n",
        "        print(\"ERROR: Tipe vectorizer tidak dikenal.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: File model Tahap 3 tidak ditemukan. Pastikan Tahap 3 sudah selesai dan menyimpan outputnya.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Gagal memuat model/vektor dari Tahap 3: {e}\")\n",
        "\n",
        "# Muat df_cases untuk Tahap 4 (juga digunakan oleh fungsi retrieve di bawah)\n",
        "df_cases = load_cases_data(processed_data_folder)\n",
        "\n",
        "# --- Definisi Ulang Fungsi retrieve dari Tahap 3 agar bisa digunakan di sini ---\n",
        "# Ini menggunakan variabel global yang baru saja dimuat: loaded_vectorizer, loaded_embedding_model, etc.\n",
        "def retrieve(query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Menemukan top-k kasus lama yang paling mirip dengan query kasus baru.\n",
        "    \"\"\"\n",
        "    print(f\"\\nMelakukan retrieval untuk query: '{query[:50]}...' (top-k={k})\")\n",
        "\n",
        "    processed_query = clean_text(query)\n",
        "    # print(f\"DEBUG: Query setelah pre-processing: '{processed_query[:50]}...'\")\n",
        "\n",
        "    query_vector = None\n",
        "    if loaded_vectorizer_type == \"TF-IDF\":\n",
        "        if loaded_vectorizer is not None:\n",
        "            query_vector = loaded_vectorizer.transform([processed_query])\n",
        "        else:\n",
        "            print(\"ERROR: TF-IDF Vectorizer tidak dimuat.\")\n",
        "            return []\n",
        "    elif loaded_vectorizer_type == \"BERT_Embedding\":\n",
        "        if loaded_embedding_model is not None:\n",
        "            query_vector = loaded_embedding_model.encode([processed_query], convert_to_tensor=True).cpu().numpy()\n",
        "        else:\n",
        "            print(\"ERROR: BERT Embedding Model tidak dimuat.\")\n",
        "            return []\n",
        "    else:\n",
        "        print(\"ERROR: Tipe vectorizer tidak dikenal atau belum dimuat.\")\n",
        "        return []\n",
        "\n",
        "    if query_vector is None or loaded_case_vectors is None:\n",
        "        print(\"ERROR: Query vector atau loaded_case_vectors kosong. Tidak bisa menghitung kemiripan.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        import scipy.sparse # Import lokal di sini untuk fungsi ini\n",
        "    except ImportError:\n",
        "        print(\"Peringatan: scipy.sparse tidak dapat diimpor. Mungkin tidak diperlukan jika tidak menggunakan TF-IDF.\")\n",
        "        pass # Lanjutkan jika tidak pakai TF-IDF\n",
        "\n",
        "    # Periksa tipe loaded_case_vectors dengan lebih aman\n",
        "    is_sparse_matrix = False\n",
        "    try:\n",
        "        # Coba akses scipy.sparse.csr_matrix, jika scipy tidak ada, ini akan skip\n",
        "        if 'scipy.sparse' in globals() and hasattr(scipy.sparse, 'csr_matrix'):\n",
        "            is_sparse_matrix = isinstance(loaded_case_vectors, scipy.sparse.csr_matrix)\n",
        "    except NameError: # Jika scipy belum diimport, atau bukan sparse matrix\n",
        "        pass\n",
        "\n",
        "    if not isinstance(loaded_case_vectors, np.ndarray) and not is_sparse_matrix:\n",
        "         print(f\"ERROR: loaded_case_vectors memiliki tipe yang tidak didukung untuk similarity: {type(loaded_case_vectors)}\")\n",
        "         return []\n",
        "\n",
        "    try:\n",
        "        similarities = cosine_similarity(query_vector, loaded_case_vectors).flatten()\n",
        "        # print(f\"DEBUG: Similarities shape: {similarities.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Gagal menghitung cosine similarity: {e}\")\n",
        "        print(f\"  Query vector shape: {query_vector.shape if query_vector is not None else 'None'}\")\n",
        "        print(f\"  Corpus vectors shape: {loaded_case_vectors.shape if loaded_case_vectors is not None else 'None'}\")\n",
        "        return []\n",
        "\n",
        "    if len(similarities) < k:\n",
        "        print(f\"Peringatan: Jumlah sampel ({len(similarities)}) kurang dari k ({k}). Mengembalikan semua sampel.\")\n",
        "        top_k_indices = np.arange(len(similarities))\n",
        "    else:\n",
        "        top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
        "\n",
        "    sorted_top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])][::-1]\n",
        "\n",
        "    results = []\n",
        "    original_valid_indices_mask = df_cases['text_pdf'].apply(lambda x: isinstance(x, str) and x.strip() != '')\n",
        "    original_valid_indices = df_cases[original_valid_indices_mask].index.tolist()\n",
        "\n",
        "    if len(original_valid_indices) != loaded_case_vectors.shape[0]:\n",
        "        print(f\"ERROR: Ketidakcocokan panjang antara original_valid_indices ({len(original_valid_indices)}) dan loaded_case_vectors ({loaded_case_vectors.shape[0]}).\")\n",
        "        print(\"Ini bisa terjadi jika ada dokumen yang dibuang setelah embedding atau kesalahan indexing.\")\n",
        "        return []\n",
        "\n",
        "    for idx_in_corpus in sorted_top_k_indices:\n",
        "        original_df_index = original_valid_indices[idx_in_corpus]\n",
        "        case_info = df_cases.loc[original_df_index]\n",
        "        similarity_score = similarities[idx_in_corpus]\n",
        "        results.append({\n",
        "            \"case_id\": case_info['nomor'],\n",
        "            \"judul\": case_info['judul'],\n",
        "            \"klasifikasi\": case_info['klasifikasi'],\n",
        "            \"similarity_score\": float(similarity_score),\n",
        "            \"link\": case_info['link']\n",
        "        })\n",
        "\n",
        "    print(f\"Retrieval selesai. Ditemukan {len(results)} kasus.\")\n",
        "    return results\n",
        "\n",
        "print(\"Fungsi utilitas dan pemuatan model Tahap 4 berhasil.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3k4APtfIP7xv",
        "outputId": "28e9c084-d775-44c8-8ddc-8ea718de396e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memuat model dan vektor dari Tahap 3...\n",
            "Tipe vectorizer yang digunakan: BERT_Embedding\n",
            "Memuat ulang SentenceTransformer model untuk BERT Embedding...\n",
            "BERT embedding model dan case vectors berhasil dimuat.\n",
            "Mencoba memuat cases.csv dari: /content/drive/MyDrive/CBR_Data/data/processed/cases.csv\n",
            "Berhasil memuat 65 kasus dari cases.csv.\n",
            "Fungsi utilitas dan pemuatan model Tahap 4 berhasil.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_solution_text(case_row: pd.Series) -> str:\n",
        "    \"\"\"\n",
        "    Mengekstrak teks \"solusi\" (amar putusan atau ringkasan dakwaan) dari baris kasus.\n",
        "    Args:\n",
        "        case_row (pd.Series): Baris (record) DataFrame dari kasus yang mirip.\n",
        "    Returns:\n",
        "        str: Teks solusi yang diekstrak.\n",
        "    \"\"\"\n",
        "    # Prioritaskan 'amar' putusan\n",
        "    if 'amar' in case_row and case_row['amar'] and case_row['amar'].strip() != 'nan':\n",
        "        return f\"Amar Putusan: {case_row['amar'].strip()}\"\n",
        "    # Jika 'amar' tidak ada atau kosong, coba 'catatan_amar'\n",
        "    elif 'catatan_amar' in case_row and case_row['catatan_amar'] and case_row['catatan_amar'].strip() != 'nan':\n",
        "        return f\"Catatan Amar: {case_row['catatan_amar'].strip()}\"\n",
        "    # Fallback ke 'ringkasan_fakta' (yang mungkin berisi dakwaan atau ringkasan)\n",
        "    elif 'ringkasan_fakta' in case_row and case_row['ringkasan_fakta'] and case_row['ringkasan_fakta'].strip() != 'nan':\n",
        "        return f\"Ringkasan Fakta (Kemungkinan Dakwaan): {case_row['ringkasan_fakta'].strip()}\"\n",
        "    # Fallback ke 'argumen_hukum_utama'\n",
        "    elif 'argumen_hukum_utama' in case_row and case_row['argumen_hukum_utama'] and case_row['argumen_hukum_utama'].strip() != 'nan':\n",
        "        return f\"Argumen Hukum Utama: {case_row['argumen_hukum_utama'].strip()}\"\n",
        "\n",
        "    return \"Solusi tidak dapat diekstrak dari kasus ini.\"\n",
        "\n",
        "print(\"Fungsi ekstraksi solusi berhasil didefinisikan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV2P8lgNP9XV",
        "outputId": "789b2ab8-3be6-4bc0-cf7b-79dd5c431330"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fungsi ekstraksi solusi berhasil didefinisikan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json # Pastikan json diimpor di sel ini juga jika belum\n",
        "\n",
        "def predict_outcome(query: str, k: int = 5) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Memprediksi solusi untuk kasus baru berdasarkan top-k kasus terkemuka.\n",
        "    Args:\n",
        "        query (str): Teks query kasus baru.\n",
        "        k (int): Jumlah kasus teratas yang akan dipertimbangkan.\n",
        "    Returns:\n",
        "        Dict[str, Any]: Dictionary berisi prediksi solusi dan top-k case IDs.\n",
        "    \"\"\"\n",
        "    print(f\"\\nMemprediksi solusi untuk query: '{query[:50]}...'\")\n",
        "\n",
        "    # Dapatkan top-k kasus termirip menggunakan fungsi retrieve dari Tahap 3\n",
        "    # Retrieve akan mengembalikan daftar dict: [{\"case_id\": ..., \"judul\": ..., \"similarity_score\": ...}]\n",
        "    top_k_cases_retrieved = retrieve(query, k=k) # <<< SINTAKSIS SITASI DIHAPUS DI SINI\n",
        "\n",
        "    if not top_k_cases_retrieved:\n",
        "        print(\"Tidak ada kasus terkemuka yang ditemukan. Tidak dapat memprediksi solusi.\")\n",
        "        return {\n",
        "            \"predicted_solution\": \"Tidak dapat memprediksi solusi karena tidak ada kasus serupa ditemukan.\",\n",
        "            \"top_k_case_ids\": [],\n",
        "            \"top_k_details\": []\n",
        "        }\n",
        "\n",
        "    solutions_with_similarity = []\n",
        "    top_k_case_ids = []\n",
        "    top_k_details = []\n",
        "\n",
        "    for case_data in top_k_cases_retrieved:\n",
        "        case_id = case_data['case_id']\n",
        "        similarity_score = case_data['similarity_score']\n",
        "        original_link = case_data['link']\n",
        "\n",
        "        # Cari baris lengkap kasus di df_cases menggunakan case_id (nomor) atau link\n",
        "        found_case_row = df_cases[df_cases['nomor'] == case_id]\n",
        "        if found_case_row.empty:\n",
        "             found_case_row = df_cases[df_cases['link'] == original_link]\n",
        "\n",
        "        if not found_case_row.empty:\n",
        "            solution_text = extract_solution_text(found_case_row.iloc[0])\n",
        "            solutions_with_similarity.append({\n",
        "                \"solution\": solution_text,\n",
        "                \"similarity\": similarity_score\n",
        "            })\n",
        "            top_k_case_ids.append(case_id)\n",
        "            top_k_details.append({\n",
        "                \"case_id\": case_id,\n",
        "                \"judul\": case_data['judul'],\n",
        "                \"klasifikasi\": case_data['klasifikasi'],\n",
        "                \"similarity_score\": similarity_score,\n",
        "                \"solution_text_preview\": solution_text[:100] + \"...\" if len(solution_text) > 100 else solution_text\n",
        "            })\n",
        "        else:\n",
        "            print(f\"Peringatan: Detail kasus lengkap untuk case_id {case_id} tidak ditemukan di df_cases.\")\n",
        "            solutions_with_similarity.append({\n",
        "                \"solution\": \"Detail kasus tidak ditemukan.\",\n",
        "                \"similarity\": similarity_score\n",
        "            })\n",
        "            top_k_case_ids.append({\n",
        "                \"case_id\": case_id,\n",
        "                \"judul\": case_data['judul'],\n",
        "                \"klasifikasi\": case_data['klasifikasi'],\n",
        "                \"similarity_score\": similarity_score,\n",
        "                \"solution_text_preview\": \"Detail kasus tidak ditemukan.\"\n",
        "            })\n",
        "\n",
        "\n",
        "    predicted_solution = \"Tidak dapat memprediksi solusi.\"\n",
        "    if solutions_with_similarity:\n",
        "        sorted_solutions = sorted(solutions_with_similarity, key=lambda x: x['similarity'], reverse=True)\n",
        "        predicted_solution = sorted_solutions[0]['solution']\n",
        "        print(f\"DEBUG: Solusi prediksi diambil dari kasus dengan skor kemiripan tertinggi ({sorted_solutions[0]['similarity']:.4f}).\")\n",
        "\n",
        "    print(\"Prediksi solusi selesai.\")\n",
        "    return {\n",
        "        \"predicted_solution\": predicted_solution,\n",
        "        \"top_k_case_ids\": top_k_case_ids,\n",
        "        \"top_k_details\": top_k_details\n",
        "    }\n",
        "\n",
        "print(\"Fungsi predict_outcome berhasil didefinisikan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avnEcZxIQAeY",
        "outputId": "8a381a33-bf54-45f0-82d5-ce2097f70a76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fungsi predict_outcome berhasil didefinisikan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Siapkan 5 contoh kasus baru untuk demo manual ---\n",
        "sample_prediction_queries = [\n",
        "    {\n",
        "        \"query_id\": \"PREDICT_Q1_Narkotika\",\n",
        "        \"text\": \"Seorang mahasiswa ditangkap dengan barang bukti ekstasi 10 butir dan terjerat tindak pidana narkotika.\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"PREDICT_Q2_Perdata\",\n",
        "        \"text\": \"Kasus sengketa hutang piutang antara dua perusahaan yang berujung pada gugatan wanprestasi di pengadilan negeri.\"\n",
        "    },\n",
        "    {\n",
        "        \"query_id\": \"PREDICT_Q3_Pidana_Lain\",\n",
        "        \"text\": \"Kasus penipuan online dengan kerugian jutaan rupiah melalui media sosial, pelaku menggunakan modus investasi bodong.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Output Path untuk hasil prediksi ---\n",
        "base_drive_path = '/content/drive/MyDrive/CBR_Data' # Definisikan ulang base_drive_path\n",
        "results_folder = create_path(os.path.join(base_drive_path, 'data/results'))\n",
        "predictions_csv_path = os.path.join(results_folder, 'predictions.csv') # Hapus sintaks sitasi yang salah di sini\n",
        "\n",
        "# --- Lakukan demo prediksi ---\n",
        "print(\"\\n--- Melakukan Demo Prediksi Solusi untuk Kasus Baru ---\")\n",
        "all_predictions_data = []\n",
        "\n",
        "if df_cases.empty:\n",
        "    print(\"Tidak dapat melakukan prediksi karena data kasus kosong. Pastikan Tahap 2 sudah berhasil.\")\n",
        "elif 'retrieve' not in locals() or not callable(retrieve):\n",
        "    print(\"Fungsi 'retrieve' tidak ditemukan. Pastikan Sel 2 sudah dijalankan dengan benar.\")\n",
        "elif 'predict_outcome' not in locals() or not callable(predict_outcome):\n",
        "    print(\"Fungsi 'predict_outcome' tidak ditemukan. Pastikan Sel 4 sudah dijalankan dengan benar.\")\n",
        "else:\n",
        "    for query_data in sample_prediction_queries:\n",
        "        query_text = query_data['text']\n",
        "        query_id = query_data['query_id']\n",
        "\n",
        "        predicted_output = predict_outcome(query_text, k=5)\n",
        "\n",
        "        print(f\"\\nQuery ID: {query_id}\")\n",
        "        print(f\"  Query Text: {query_text[:100]}...\")\n",
        "        print(f\"  PREDIKSI SOLUSI: {predicted_output['predicted_solution'][:200]}...\")\n",
        "        print(f\"  Top {len(predicted_output['top_k_case_ids'])} Kasus Termirip (IDs): {predicted_output['top_k_case_ids']}\")\n",
        "\n",
        "        all_predictions_data.append({\n",
        "            \"query_id\": query_id,\n",
        "            \"predicted_solution\": predicted_output['predicted_solution'],\n",
        "            \"top_5_case_ids\": \", \".join(predicted_output['top_k_case_ids']),\n",
        "            \"top_5_details\": json.dumps(predicted_output['top_k_details'], ensure_ascii=False)\n",
        "        })\n",
        "\n",
        "    if all_predictions_data:\n",
        "        try:\n",
        "            df_predictions = pd.DataFrame(all_predictions_data)\n",
        "            df_predictions.to_csv(predictions_csv_path, index=False, encoding='utf-8')\n",
        "            print(f\"\\nHasil prediksi berhasil disimpan ke: {predictions_csv_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Gagal menyimpan predictions.csv: {e}\")\n",
        "    else:\n",
        "        print(\"Tidak ada data prediksi untuk disimpan.\")\n",
        "\n",
        "\n",
        "print(\"\\nPROSES TAHAP 4 (SOLUTION REUSE) SELESAI.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L35SfEd1QDXE",
        "outputId": "2b86c728-e360-49b9-8c9b-c3fdc5aaf674"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/drive/MyDrive/CBR_Data/data/results' sudah ada di Google Drive.\n",
            "\n",
            "--- Melakukan Demo Prediksi Solusi untuk Kasus Baru ---\n",
            "\n",
            "Memprediksi solusi untuk query: 'Seorang mahasiswa ditangkap dengan barang bukti ek...'\n",
            "\n",
            "Melakukan retrieval untuk query: 'Seorang mahasiswa ditangkap dengan barang bukti ek...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "DEBUG: Solusi prediksi diambil dari kasus dengan skor kemiripan tertinggi (0.1897).\n",
            "Prediksi solusi selesai.\n",
            "\n",
            "Query ID: PREDICT_Q1_Narkotika\n",
            "  Query Text: Seorang mahasiswa ditangkap dengan barang bukti ekstasi 10 butir dan terjerat tindak pidana narkotik...\n",
            "  PREDIKSI SOLUSI: Amar Putusan: Lain-lain...\n",
            "  Top 5 Kasus Termirip (IDs): ['154/PID.SUS/2025/PT MAM', '1109/PID.SUS/2025/PT MDN', '304/PID.SUS/2025/PT PBR', '172/PID.SUS/2025/PT TJK', '326/PID.SUS/2025/PT PBR']\n",
            "\n",
            "Memprediksi solusi untuk query: 'Kasus sengketa hutang piutang antara dua perusahaa...'\n",
            "\n",
            "Melakukan retrieval untuk query: 'Kasus sengketa hutang piutang antara dua perusahaa...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "DEBUG: Solusi prediksi diambil dari kasus dengan skor kemiripan tertinggi (0.2645).\n",
            "Prediksi solusi selesai.\n",
            "\n",
            "Query ID: PREDICT_Q2_Perdata\n",
            "  Query Text: Kasus sengketa hutang piutang antara dua perusahaan yang berujung pada gugatan wanprestasi di pengad...\n",
            "  PREDIKSI SOLUSI: Amar Putusan: Lain-lain...\n",
            "  Top 5 Kasus Termirip (IDs): ['270/PID.SUS/2025/PT PDG', '263/PID.SUS/2025/PT PDG', '46/PID.SUS/2025/PT TJS', '326/PID.SUS/2025/PT PBR', '326/PID.SUS/2025/PT PBR']\n",
            "\n",
            "Memprediksi solusi untuk query: 'Kasus penipuan online dengan kerugian jutaan rupia...'\n",
            "\n",
            "Melakukan retrieval untuk query: 'Kasus penipuan online dengan kerugian jutaan rupia...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "DEBUG: Solusi prediksi diambil dari kasus dengan skor kemiripan tertinggi (0.1746).\n",
            "Prediksi solusi selesai.\n",
            "\n",
            "Query ID: PREDICT_Q3_Pidana_Lain\n",
            "  Query Text: Kasus penipuan online dengan kerugian jutaan rupiah melalui media sosial, pelaku menggunakan modus i...\n",
            "  PREDIKSI SOLUSI: Amar Putusan: Lain-lain...\n",
            "  Top 5 Kasus Termirip (IDs): ['484/PID.SUS/2025/PT SMG', '484/PID.SUS/2025/PT SMG', '484/PID.SUS/2025/PT SMG', '326/PID.SUS/2025/PT PBR', '326/PID.SUS/2025/PT PBR']\n",
            "\n",
            "Hasil prediksi berhasil disimpan ke: /content/drive/MyDrive/CBR_Data/data/results/predictions.csv\n",
            "\n",
            "PROSES TAHAP 4 (SOLUTION REUSE) SELESAI.\n"
          ]
        }
      ]
    }
  ]
}