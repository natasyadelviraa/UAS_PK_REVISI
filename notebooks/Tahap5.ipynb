{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOLqFGsxShyb",
        "outputId": "5fab8e9d-4fde-42a4-e4cf-13932a08d857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Menginstal library yang dibutuhkan untuk Tahap 5...\n",
            "Instalasi library selesai.\n",
            "Menghubungkan Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive terhubung.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"UAS_Penalaran_Komputer_Tahap_5.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1C_YOUR_NOTEBOOK_ID_HERE\n",
        "\"\"\"\n",
        "\n",
        "# Instalasi Library\n",
        "print(\"Menginstal library yang dibutuhkan untuk Tahap 5...\")\n",
        "!pip install pandas scikit-learn numpy joblib transformers sentence-transformers > /dev/null 2>&1\n",
        "print(\"Instalasi library selesai.\")\n",
        "\n",
        "# Koneksi Google Drive\n",
        "print(\"Menghubungkan Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive terhubung.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import joblib # Untuk memuat model TF-IDF\n",
        "import numpy as np # Untuk memuat BERT embeddings\n",
        "from sentence_transformers import SentenceTransformer # Untuk model embedding BERT\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Untuk vectorizer TF-IDF\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re # Untuk clean_text\n",
        "import json # Untuk memuat queries.json\n",
        "from typing import List, Dict, Any, Union, Tuple\n",
        "\n",
        "# --- Fungsi untuk membuat jalur (diulang agar tersedia di Tahap 5) ---\n",
        "def create_path(folder_name: str) -> str:\n",
        "    path = os.path.join('/content/drive/MyDrive/', folder_name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "        print(f\"Folder '{path}' berhasil dibuat di Google Drive.\")\n",
        "    else:\n",
        "        print(f\"Folder '{path}' sudah ada di Google Drive.\")\n",
        "    return path\n",
        "\n",
        "# --- Fungsi clean_text (diulang agar tersedia di Tahap 5) ---\n",
        "def clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = re.sub(r'M a h ka m a h A g u n g R e p u blik In d o n esia\\n', '', text)\n",
        "    text = re.sub(r'Disclaimer\\n', '', text)\n",
        "    text = re.sub(r'Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n', '', text)\n",
        "    text = re.sub(r'pelaksanaan fungsi peradilan\\. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu\\.\\n', '', text)\n",
        "    text = re.sub(r'Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n', '', text)\n",
        "    text = re.sub(r'Email : kepaniteraan@mahkamahagung\\.go\\.id\\s+Telp : 021-384 3348 \\(ext\\.318\\)\\n', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# --- Fungsi untuk memuat cases.csv (diulang agar tersedia di Tahap 5) ---\n",
        "def load_cases_data(processed_data_folder: str) -> pd.DataFrame:\n",
        "    cases_csv_path = os.path.join(processed_data_folder, 'cases.csv')\n",
        "    print(f\"Mencoba memuat cases.csv dari: {cases_csv_path}\")\n",
        "    if not os.path.exists(cases_csv_path):\n",
        "        print(f\"ERROR: File cases.csv TIDAK DITEMUKAN di '{cases_csv_path}'. Pastikan Tahap 2 sudah selesai dengan benar.\")\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        df = pd.read_csv(cases_csv_path)\n",
        "        if df.empty:\n",
        "            print(f\"Peringatan: File cases.csv ditemukan tetapi KOSONG.\")\n",
        "        else:\n",
        "            print(f\"Berhasil memuat {len(df)} kasus dari cases.csv.\")\n",
        "            for col in ['judul', 'ringkasan_fakta', 'argumen_hukum_utama', 'text_pdf', 'amar', 'amar_lainnya', 'catatan_amar']:\n",
        "                if col in df.columns:\n",
        "                    df[col] = df[col].astype(str).fillna('')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Gagal memuat cases.csv: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# --- Fungsi ekstraksi solusi (diulang agar tersedia di Tahap 5) ---\n",
        "def extract_solution_text(case_row: pd.Series) -> str:\n",
        "    if 'amar' in case_row and case_row['amar'] and case_row['amar'].strip() != 'nan':\n",
        "        return f\"Amar Putusan: {case_row['amar'].strip()}\"\n",
        "    elif 'catatan_amar' in case_row and case_row['catatan_amar'] and case_row['catatan_amar'].strip() != 'nan':\n",
        "        return f\"Catatan Amar: {case_row['catatan_amar'].strip()}\"\n",
        "    elif 'ringkasan_fakta' in case_row and case_row['ringkasan_fakta'] and case_row['ringkasan_fakta'].strip() != 'nan':\n",
        "        return f\"Ringkasan Fakta (Kemungkinan Dakwaan): {case_row['ringkasan_fakta'].strip()}\"\n",
        "    elif 'argumen_hukum_utama' in case_row and case_row['argumen_hukum_utama'] and case_row['argumen_hukum_utama'].strip() != 'nan':\n",
        "        return f\"Argumen Hukum Utama: {case_row['argumen_hukum_utama'].strip()}\"\n",
        "    return \"Solusi tidak dapat diekstrak dari kasus ini.\"\n",
        "\n",
        "\n",
        "# --- Load Model dan Vektor dari Tahap 3 ---\n",
        "# Konfigurasi Jalur\n",
        "base_drive_path = '/content/drive/MyDrive/CBR_Data'\n",
        "models_output_folder = os.path.join(base_drive_path, 'models')\n",
        "processed_data_folder = os.path.join(base_drive_path, 'data/processed') # Untuk load cases.csv\n",
        "eval_data_folder = create_path(os.path.join(base_drive_path, 'data/eval')) # Untuk menyimpan hasil evaluasi\n",
        "\n",
        "# Inisialisasi variabel global untuk Tahap 5\n",
        "loaded_vectorizer = None # TfidfVectorizer jika TF-IDF\n",
        "loaded_embedding_model = None # SentenceTransformer jika BERT\n",
        "loaded_case_vectors = None # Numpy array atau sparse matrix dari kasus\n",
        "loaded_vectorizer_type = None # \"TF-IDF\" atau \"BERT_Embedding\"\n",
        "df_cases = pd.DataFrame() # Inisialisasi df_cases\n",
        "\n",
        "print(\"Memuat model dan vektor dari Tahap 3...\")\n",
        "try:\n",
        "    with open(os.path.join(models_output_folder, 'vectorizer_type.txt'), 'r') as f:\n",
        "        loaded_vectorizer_type = f.read().strip()\n",
        "    print(f\"Tipe vectorizer yang digunakan: {loaded_vectorizer_type}\")\n",
        "\n",
        "    if loaded_vectorizer_type == \"TF-IDF\":\n",
        "        loaded_vectorizer = joblib.load(os.path.join(models_output_folder, 'tfidf_vectorizer.joblib'))\n",
        "        loaded_case_vectors = joblib.load(os.path.join(models_output_folder, 'tfidf_case_vectors.joblib'))\n",
        "        print(\"TF-IDF vectorizer dan case vectors berhasil dimuat.\")\n",
        "    elif loaded_vectorizer_type == \"BERT_Embedding\":\n",
        "        print(\"Memuat ulang SentenceTransformer model untuk BERT Embedding...\")\n",
        "        loaded_embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        loaded_case_vectors = np.load(os.path.join(models_output_folder, 'bert_case_vectors.npy'))\n",
        "        print(\"BERT embedding model dan case vectors berhasil dimuat.\")\n",
        "    else:\n",
        "        print(\"ERROR: Tipe vectorizer tidak dikenal.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: File model Tahap 3 tidak ditemukan. Pastikan Tahap 3 sudah selesai dan menyimpan outputnya.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Gagal memuat model/vektor dari Tahap 3: {e}\")\n",
        "\n",
        "# Muat df_cases untuk Tahap 5\n",
        "df_cases = load_cases_data(processed_data_folder)\n",
        "\n",
        "# --- Definisi Ulang Fungsi retrieve dari Tahap 3/4 ---\n",
        "# Menggunakan variabel global yang baru saja dimuat.\n",
        "def retrieve(query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
        "    print(f\"\\nMelakukan retrieval untuk query: '{query[:50]}...' (top-k={k})\")\n",
        "\n",
        "    processed_query = clean_text(query)\n",
        "    query_vector = None\n",
        "\n",
        "    if loaded_vectorizer_type == \"TF-IDF\":\n",
        "        if loaded_vectorizer is not None:\n",
        "            query_vector = loaded_vectorizer.transform([processed_query])\n",
        "        else:\n",
        "            print(\"ERROR: TF-IDF Vectorizer tidak dimuat.\")\n",
        "            return []\n",
        "    elif loaded_vectorizer_type == \"BERT_Embedding\":\n",
        "        if loaded_embedding_model is not None:\n",
        "            query_vector = loaded_embedding_model.encode([processed_query], convert_to_tensor=True).cpu().numpy()\n",
        "        else:\n",
        "            print(\"ERROR: BERT Embedding Model tidak dimuat.\")\n",
        "            return []\n",
        "    else:\n",
        "        print(\"ERROR: Tipe vectorizer tidak dikenal atau belum dimuat.\")\n",
        "        return []\n",
        "\n",
        "    if query_vector is None or loaded_case_vectors is None:\n",
        "        print(\"ERROR: Query vector atau loaded_case_vectors kosong. Tidak bisa menghitung kemiripan.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        import scipy.sparse # Import lokal di sini\n",
        "    except ImportError:\n",
        "        pass\n",
        "\n",
        "    is_sparse_matrix = False\n",
        "    try:\n",
        "        if 'scipy.sparse' in globals() and hasattr(scipy.sparse, 'csr_matrix'):\n",
        "            is_sparse_matrix = isinstance(loaded_case_vectors, scipy.sparse.csr_matrix)\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    if not isinstance(loaded_case_vectors, np.ndarray) and not is_sparse_matrix:\n",
        "         print(f\"ERROR: loaded_case_vectors memiliki tipe yang tidak didukung untuk similarity: {type(loaded_case_vectors)}\")\n",
        "         return []\n",
        "\n",
        "    try:\n",
        "        similarities = cosine_similarity(query_vector, loaded_case_vectors).flatten()\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: Gagal menghitung cosine similarity: {e}\")\n",
        "        print(f\"  Query vector shape: {query_vector.shape if query_vector is not None else 'None'}\")\n",
        "        print(f\"  Corpus vectors shape: {loaded_case_vectors.shape if loaded_case_vectors is not None else 'None'}\")\n",
        "        return []\n",
        "\n",
        "    if len(similarities) < k:\n",
        "        print(f\"Peringatan: Jumlah sampel ({len(similarities)}) kurang dari k ({k}). Mengembalikan semua sampel.\")\n",
        "        top_k_indices = np.arange(len(similarities))\n",
        "    else:\n",
        "        top_k_indices = np.argpartition(similarities, -k)[-k:]\n",
        "\n",
        "    sorted_top_k_indices = top_k_indices[np.argsort(similarities[top_k_indices])][::-1]\n",
        "\n",
        "    results = []\n",
        "    original_valid_indices_mask = df_cases['text_pdf'].apply(lambda x: isinstance(x, str) and x.strip() != '')\n",
        "    original_valid_indices = df_cases[original_valid_indices_mask].index.tolist()\n",
        "\n",
        "    if len(original_valid_indices) != loaded_case_vectors.shape[0]:\n",
        "        print(f\"ERROR: Ketidakcocokan panjang antara original_valid_indices ({len(original_valid_indices)}) dan loaded_case_vectors ({loaded_case_vectors.shape[0]}).\")\n",
        "        print(\"Ini bisa terjadi jika ada dokumen yang dibuang setelah embedding atau kesalahan indexing.\")\n",
        "        return []\n",
        "\n",
        "    for idx_in_corpus in sorted_top_k_indices:\n",
        "        original_df_index = original_valid_indices[idx_in_corpus]\n",
        "        case_info = df_cases.loc[original_df_index]\n",
        "        similarity_score = similarities[idx_in_corpus]\n",
        "        results.append({\n",
        "            \"case_id\": case_info['nomor'],\n",
        "            \"judul\": case_info['judul'],\n",
        "            \"klasifikasi\": case_info['klasifikasi'],\n",
        "            \"similarity_score\": float(similarity_score),\n",
        "            \"link\": case_info['link']\n",
        "        })\n",
        "\n",
        "    print(f\"Retrieval selesai. Ditemukan {len(results)} kasus.\")\n",
        "    return results\n",
        "\n",
        "# --- Definisi Ulang Fungsi predict_outcome dari Tahap 4 ---\n",
        "# Menggunakan variabel global yang baru saja dimuat.\n",
        "def predict_outcome(query: str, k: int = 5) -> Dict[str, Any]:\n",
        "    print(f\"\\nMemprediksi solusi untuk query: '{query[:50]}...'\")\n",
        "\n",
        "    top_k_cases_retrieved = retrieve(query, k=k)\n",
        "\n",
        "    if not top_k_cases_retrieved:\n",
        "        print(\"Tidak ada kasus terkemuka yang ditemukan. Tidak dapat memprediksi solusi.\")\n",
        "        return {\n",
        "            \"predicted_solution\": \"Tidak dapat memprediksi solusi karena tidak ada kasus serupa ditemukan.\",\n",
        "            \"top_k_case_ids\": [],\n",
        "            \"top_k_details\": []\n",
        "        }\n",
        "\n",
        "    solutions_with_similarity = []\n",
        "    top_k_case_ids = []\n",
        "    top_k_details = []\n",
        "\n",
        "    for case_data in top_k_cases_retrieved:\n",
        "        case_id = case_data['case_id']\n",
        "        similarity_score = case_data['similarity_score']\n",
        "        original_link = case_data['link']\n",
        "\n",
        "        found_case_row = df_cases[df_cases['nomor'] == case_id]\n",
        "        if found_case_row.empty:\n",
        "             found_case_row = df_cases[df_cases['link'] == original_link]\n",
        "\n",
        "        if not found_case_row.empty:\n",
        "            solution_text = extract_solution_text(found_case_row.iloc[0])\n",
        "            solutions_with_similarity.append({\n",
        "                \"solution\": solution_text,\n",
        "                \"similarity\": similarity_score\n",
        "            })\n",
        "            top_k_case_ids.append(case_id)\n",
        "            top_k_details.append({\n",
        "                \"case_id\": case_id,\n",
        "                \"judul\": case_data['judul'],\n",
        "                \"klasifikasi\": case_data['klasifikasi'],\n",
        "                \"similarity_score\": similarity_score,\n",
        "                \"solution_text_preview\": solution_text[:100] + \"...\" if len(solution_text) > 100 else solution_text\n",
        "            })\n",
        "        else:\n",
        "            print(f\"Peringatan: Detail kasus lengkap untuk case_id {case_id} tidak ditemukan di df_cases.\")\n",
        "            solutions_with_similarity.append({\n",
        "                \"solution\": \"Detail kasus tidak ditemukan.\",\n",
        "                \"similarity\": similarity_score\n",
        "            })\n",
        "            top_k_case_ids.append(case_id) # Pastikan hanya case_id, bukan dict\n",
        "            top_k_details.append({\n",
        "                \"case_id\": case_id,\n",
        "                \"judul\": case_data['judul'],\n",
        "                \"klasifikasi\": case_data['klasifikasi'],\n",
        "                \"similarity_score\": similarity_score,\n",
        "                \"solution_text_preview\": \"Detail kasus tidak ditemukan.\"\n",
        "            })\n",
        "\n",
        "\n",
        "    predicted_solution = \"Tidak dapat memprediksi solusi.\"\n",
        "    if solutions_with_similarity:\n",
        "        sorted_solutions = sorted(solutions_with_similarity, key=lambda x: x['similarity'], reverse=True)\n",
        "        predicted_solution = sorted_solutions[0]['solution']\n",
        "        print(f\"DEBUG: Solusi prediksi diambil dari kasus dengan skor kemiripan tertinggi ({sorted_solutions[0]['similarity']:.4f}).\")\n",
        "\n",
        "    print(\"Prediksi solusi selesai.\")\n",
        "    return {\n",
        "        \"predicted_solution\": predicted_solution,\n",
        "        \"top_k_case_ids\": top_k_case_ids,\n",
        "        \"top_k_details\": top_k_details\n",
        "    }\n",
        "\n",
        "print(\"Semua fungsi utilitas dan model Tahap 5 berhasil didefinisikan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHgwpfd7Sm26",
        "outputId": "b3339913-c505-47f5-d6fa-7fda33b5b45c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/drive/MyDrive/CBR_Data/data/eval' sudah ada di Google Drive.\n",
            "Memuat model dan vektor dari Tahap 3...\n",
            "Tipe vectorizer yang digunakan: BERT_Embedding\n",
            "Memuat ulang SentenceTransformer model untuk BERT Embedding...\n",
            "BERT embedding model dan case vectors berhasil dimuat.\n",
            "Mencoba memuat cases.csv dari: /content/drive/MyDrive/CBR_Data/data/processed/cases.csv\n",
            "Berhasil memuat 65 kasus dari cases.csv.\n",
            "Semua fungsi utilitas dan model Tahap 5 berhasil didefinisikan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import json # Untuk memuat queries.json\n",
        "\n",
        "def eval_retrieval(queries_results: List[Dict[str, Any]], k: int = 5) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Mengevaluasi performa retrieval (Precision@k, Recall@k, F1-score).\n",
        "    Args:\n",
        "        queries_results (List[Dict[str, Any]]): Daftar hasil query, masing-masing dengan 'query_id', 'query_text', 'ground_truth_case_id', 'retrieved_cases'.\n",
        "                                                Ini adalah format output saat mengumpulkan retrieval_test_results_for_eval di Sel 4.\n",
        "        k (int): Jumlah top-k item yang akan dievaluasi.\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame berisi metrik evaluasi per query.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Memulai Evaluasi Retrieval (k={k}) ---\")\n",
        "    metrics_list = [] # <<< PERBAIKAN DI SINI: Pastikan ini adalah 'metrics_list'\n",
        "\n",
        "    # Pastikan data dan model yang dibutuhkan tersedia\n",
        "    if df_cases.empty or loaded_case_vectors is None or df_cases['text_pdf'].count() == 0:\n",
        "        print(\"ERROR: df_cases kosong atau vektor kasus tidak dimuat/kosong. Tidak dapat melakukan evaluasi retrieval.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for query_data in queries_results: # Menggunakan queries_results yang sudah berisi retrieved_cases\n",
        "        query_id = query_data['query_id']\n",
        "        query_text = query_data['query_text']\n",
        "        ground_truth_ids = set(query_data.get('ground_truth_case_id', [])) # Ground truth bisa kosong\n",
        "        retrieved_cases = query_data.get('retrieved_cases', []) # Ambil hasil retrieve yang sudah ada\n",
        "\n",
        "        print(f\"  Mengevaluasi Query ID: {query_id}\")\n",
        "\n",
        "        retrieved_ids = {case['case_id'] for case in retrieved_cases}\n",
        "\n",
        "        # Hitung True Positives (TP) - item yang relevan DAN berhasil diretriev\n",
        "        true_positives = len(ground_truth_ids.intersection(retrieved_ids))\n",
        "\n",
        "        # Hitung False Positives (FP) - item yang diretriev tapi tidak relevan\n",
        "        false_positives = len(retrieved_ids) - true_positives\n",
        "\n",
        "        # Hitung False Negatives (FN) - item yang relevan tapi TIDAK berhasil diretriev\n",
        "        false_negatives = len(ground_truth_ids) - true_positives\n",
        "\n",
        "        # Presisi@k: Berapa persen dari yang diretriev adalah relevan\n",
        "        precision_at_k = true_positives / k if k > 0 else 0.0\n",
        "\n",
        "        # Recall@k: Berapa persen dari yang relevan berhasil diretriev\n",
        "        recall_at_k = true_positives / len(ground_truth_ids) if len(ground_truth_ids) > 0 else 0.0\n",
        "\n",
        "        # F1-score: Harmonic mean dari precision dan recall\n",
        "        f1_at_k = (2 * precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) if (precision_at_k + recall_at_k) > 0 else 0.0\n",
        "\n",
        "        metrics_list.append({\n",
        "            \"query_id\": query_id,\n",
        "            \"precision_at_k\": precision_at_k,\n",
        "            \"recall_at_k\": recall_at_k,\n",
        "            \"f1_score_at_k\": f1_at_k,\n",
        "            \"true_positives\": true_positives,\n",
        "            \"false_positives\": false_positives,\n",
        "            \"false_negatives\": false_negatives,\n",
        "            \"num_retrieved\": len(retrieved_ids),\n",
        "            \"num_ground_truth\": len(ground_truth_ids)\n",
        "        })\n",
        "\n",
        "    if not metrics_list: # Pemeriksaan ini menjadi aman setelah inisialisasi\n",
        "        print(\"Peringatan: Tidak ada hasil evaluasi yang dikumpulkan. Mengembalikan DataFrame kosong.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_metrics = pd.DataFrame(metrics_list)\n",
        "    print(\"\\nEvaluasi Retrieval Selesai.\")\n",
        "    return df_metrics\n",
        "\n",
        "# --- Fungsi opsional untuk evaluasi prediksi (lebih kompleks, jika target prediksi adalah label diskrit) ---\n",
        "def eval_prediction(predictions: List[Dict[str, Any]], ground_truth_solutions: Dict[str, str] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    (Opsional/Lanjutan) Mengevaluasi performa prediksi solusi.\n",
        "    Membutuhkan ground truth solusi yang eksplisit untuk setiap query,\n",
        "    yang seringkali sulit didapatkan untuk teks bebas.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Memulai Evaluasi Prediksi (Analisis Kualitatif/Sederhana) ---\")\n",
        "    print(\"Implementasi ini memerlukan ground truth solusi yang terstruktur untuk setiap query agar metrik kuantitatif bisa dihitung.\")\n",
        "    print(\"Untuk saat ini, akan membuat tabel hasil prediksi untuk analisis manual/kualitatif.\")\n",
        "\n",
        "    prediction_metrics_list = []\n",
        "    for pred in predictions:\n",
        "        is_predicted_successfully = \"True\" if pred['predicted_solution'].strip() != \"Tidak dapat memprediksi solusi karena tidak ada kasus serupa ditemukan.\" and pred['predicted_solution'].strip() != \"\" else \"False\"\n",
        "        prediction_metrics_list.append({\n",
        "            \"query_id\": pred['query_id'],\n",
        "            \"query_text_preview\": pred['query_text'][:100] + \"...\" if len(pred['query_text']) > 100 else pred['query_text'],\n",
        "            \"predicted_solution_status\": is_predicted_successfully,\n",
        "            \"predicted_solution_preview\": pred['predicted_solution'][:200] + \"...\" if len(pred['predicted_solution']) > 200 else pred['predicted_solution'],\n",
        "            \"top_5_case_ids\": pred['top_5_case_ids'],\n",
        "            \"top_5_details_json\": pred['top_5_details_json'] # Simpan juga detail lengkap\n",
        "        })\n",
        "\n",
        "    df_prediction_metrics = pd.DataFrame(prediction_metrics_list)\n",
        "    print(\"Evaluasi Prediksi Selesai (Membuat Dataframe untuk Analisis Kualitatif).\")\n",
        "    return df_prediction_metrics\n",
        "\n",
        "print(\"Fungsi evaluasi retrieval dan prediksi berhasil didefinisikan.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcvyW3-FSrGI",
        "outputId": "f79f141f-43f2-4777-92a8-066fba4ddd7d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fungsi evaluasi retrieval dan prediksi berhasil didefinisikan.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json # Pastikan json diimpor di sel ini juga\n",
        "\n",
        "# --- Output Paths ---\n",
        "base_drive_path = '/content/drive/MyDrive/CBR_Data'\n",
        "eval_data_folder = create_path(os.path.join(base_drive_path, 'data/eval'))\n",
        "results_folder = create_path(os.path.join(base_drive_path, 'data/results')) # Folder hasil Tahap 4\n",
        "\n",
        "retrieval_metrics_csv_path = os.path.join(eval_data_folder, 'retrieval_metrics.csv')\n",
        "prediction_metrics_csv_path = os.path.join(eval_data_folder, 'prediction_metrics.csv') # Sesuai tugas\n",
        "\n",
        "# --- Load queries.json dari Tahap 3 ---\n",
        "# 'sample_queries_from_json' berisi data awal dari queries.json (query_id, text, ground_truth_case_id)\n",
        "sample_queries_from_json = []\n",
        "try:\n",
        "    if os.path.exists(os.path.join(eval_data_folder, 'queries.json')):\n",
        "        with open(os.path.join(eval_data_folder, 'queries.json'), 'r', encoding='utf-8') as f:\n",
        "            sample_queries_from_json = json.load(f)\n",
        "        print(f\"Berhasil memuat {len(sample_queries_from_json)} query dari queries.json.\")\n",
        "    else:\n",
        "        print(f\"ERROR: File queries.json TIDAK DITEMUKAN di '{os.path.join(eval_data_folder, 'queries.json')}'. Pastikan Tahap 3 sudah menyimpan query uji.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Gagal memuat queries.json: {e}\")\n",
        "\n",
        "# --- Siapkan data untuk evaluasi retrieval: Jalankan retrieval untuk setiap query.json ---\n",
        "retrieval_test_results_for_eval = [] # Akan diisi dengan data yang diperlukan eval_retrieval\n",
        "prediction_results_for_csv_output = [] # Akan diisi dengan data untuk predictions.csv\n",
        "\n",
        "if (not df_cases.empty and loaded_case_vectors is not None and df_cases['text_pdf'].count() > 0 and\n",
        "    'retrieve' in globals() and callable(retrieve) and\n",
        "    'predict_outcome' in globals() and callable(predict_outcome) and\n",
        "    sample_queries_from_json):\n",
        "\n",
        "    print(\"\\n--- Melakukan Retrieval dan Prediksi untuk Mengumpulkan Hasil Evaluasi ---\")\n",
        "    for query_data in sample_queries_from_json: # Iterasi dari queries.json\n",
        "        query_id = query_data['query_id']\n",
        "        query_text = query_data['text'] # Ambil 'text' dari queries.json\n",
        "        ground_truth_ids = query_data.get('ground_truth_case_id', [])\n",
        "\n",
        "        print(f\"  Memproses Query ID: {query_id}\")\n",
        "\n",
        "        # 1. Lakukan Retrieval dan simpan untuk evaluasi retrieval\n",
        "        try:\n",
        "            retrieved_cases = retrieve(query_text, k=5)\n",
        "            retrieval_test_results_for_eval.append({\n",
        "                \"query_id\": query_id,\n",
        "                \"query_text\": query_text, # Simpan 'query_text' di sini\n",
        "                \"ground_truth_case_id\": ground_truth_ids,\n",
        "                \"retrieved_cases\": retrieved_cases\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"  ERROR: Gagal melakukan retrieval untuk query {query_id} (saat evaluasi): {e}\")\n",
        "            retrieval_test_results_for_eval.append({\n",
        "                \"query_id\": query_id, \"query_text\": query_text, \"ground_truth_case_id\": ground_truth_ids,\n",
        "                \"retrieved_cases\": [], \"retrieval_error\": str(e)\n",
        "            })\n",
        "\n",
        "        # 2. Lakukan Prediksi dan simpan untuk predictions.csv\n",
        "        try:\n",
        "            predicted_output = predict_outcome(query_text, k=5)\n",
        "            prediction_results_for_csv_output.append({\n",
        "                \"query_id\": query_id,\n",
        "                \"query_text\": query_text, # Simpan query text lengkap\n",
        "                \"predicted_solution\": predicted_output['predicted_solution'],\n",
        "                \"top_5_case_ids\": \", \".join(predicted_output['top_k_case_ids']),\n",
        "                \"top_5_details_json\": json.dumps(predicted_output['top_k_details'], ensure_ascii=False)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"  ERROR: Gagal melakukan prediksi untuk query {query_id} (saat evaluasi): {e}\")\n",
        "            prediction_results_for_csv_output.append({\n",
        "                \"query_id\": query_id, \"query_text\": query_text,\n",
        "                \"predicted_solution\": \"ERROR: \" + str(e),\n",
        "                \"top_5_case_ids\": \"\", \"top_5_details_json\": \"\"\n",
        "            })\n",
        "\n",
        "    # --- Evaluasi Retrieval dan Simpan Metrik ---\n",
        "    df_retrieval_metrics = pd.DataFrame()\n",
        "    if retrieval_test_results_for_eval:\n",
        "        df_retrieval_metrics = eval_retrieval(retrieval_test_results_for_eval, k=5)\n",
        "        if not df_retrieval_metrics.empty:\n",
        "            avg_precision = df_retrieval_metrics['precision_at_k'].mean()\n",
        "            avg_recall = df_retrieval_metrics['recall_at_k'].mean()\n",
        "            avg_f1 = df_retrieval_metrics['f1_score_at_k'].mean()\n",
        "\n",
        "            print(\"\\n--- Ringkasan Metrik Retrieval ---\")\n",
        "            print(f\"Precision@5 Rata-rata: {avg_precision:.4f}\")\n",
        "            print(f\"Recall@5 Rata-rata:    {avg_recall:.4f}\")\n",
        "            print(f\"F1-Score@5 Rata-rata:  {avg_f1:.4f}\")\n",
        "\n",
        "            try:\n",
        "                df_retrieval_metrics.to_csv(retrieval_metrics_csv_path, index=False, encoding='utf-8')\n",
        "                print(f\"\\nMetrik retrieval berhasil disimpan ke: {retrieval_metrics_csv_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: Gagal menyimpan retrieval_metrics.csv: {e}\")\n",
        "        else:\n",
        "            print(\"DataFrame metrik retrieval kosong. Tidak ada hasil evaluasi untuk disimpan.\")\n",
        "    else:\n",
        "        print(\"Tidak ada hasil retrieval untuk dievaluasi.\")\n",
        "\n",
        "\n",
        "    # --- Simpan Hasil Prediksi untuk Analisis Kualitatif ---\n",
        "    df_prediction_metrics = pd.DataFrame()\n",
        "    if prediction_results_for_csv_output:\n",
        "        # Panggil eval_prediction dengan data yang sudah terkumpul\n",
        "        df_prediction_metrics = eval_prediction(prediction_results_for_csv_output)\n",
        "        if not df_prediction_metrics.empty:\n",
        "            try:\n",
        "                df_prediction_metrics.to_csv(prediction_metrics_csv_path, index=False, encoding='utf-8')\n",
        "                print(f\"\\nHasil prediksi (untuk analisis kualitatif) berhasil disimpan ke: {prediction_metrics_csv_path}\")\n",
        "                print(\"\\nUntuk analisis kegagalan model (error analysis), periksa file prediction_metrics.csv secara manual.\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR: Gagal menyimpan prediction_metrics.csv: {e}\")\n",
        "        else:\n",
        "            print(\"DataFrame metrik prediksi kosong.\")\n",
        "    else:\n",
        "        print(\"Tidak ada data prediksi untuk disimpan.\")\n",
        "\n",
        "else:\n",
        "    print(\"Tidak dapat melanjutkan eksekusi evaluasi. Pastikan data kasus, model/vektor, fungsi retrieve/predict_outcome, dan sample_queries tersedia.\")\n",
        "\n",
        "print(\"\\nPROSES TAHAP 5 (MODEL EVALUATION) SELESAI.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBrrQWwESvaz",
        "outputId": "cf28ec48-edb3-4ea7-d816-db656d88a30a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder '/content/drive/MyDrive/CBR_Data/data/eval' sudah ada di Google Drive.\n",
            "Folder '/content/drive/MyDrive/CBR_Data/data/results' sudah ada di Google Drive.\n",
            "Berhasil memuat 5 query dari queries.json.\n",
            "\n",
            "--- Melakukan Retrieval dan Prediksi untuk Mengumpulkan Hasil Evaluasi ---\n",
            "  Memproses Query ID: Q1_Narkotika_1\n",
            "\n",
            "Melakukan retrieval untuk query: 'Seorang terdakwa ditangkap karena memiliki 5 gram ...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "\n",
            "Memprediksi solusi untuk query: 'Seorang terdakwa ditangkap karena memiliki 5 gram ...'\n",
            "\n",
            "Melakukan retrieval untuk query: 'Seorang terdakwa ditangkap karena memiliki 5 gram ...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "DEBUG: Solusi prediksi diambil dari kasus dengan skor kemiripan tertinggi (0.2941).\n",
            "Prediksi solusi selesai.\n",
            "  Memproses Query ID: Q2_Narkotika_2\n",
            "\n",
            "Melakukan retrieval untuk query: 'Kasus peredaran gelap narkotika jenis ganja yang m...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "\n",
            "Memprediksi solusi untuk query: 'Kasus peredaran gelap narkotika jenis ganja yang m...'\n",
            "\n",
            "Melakukan retrieval untuk query: 'Kasus peredaran gelap narkotika jenis ganja yang m...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "DEBUG: Solusi prediksi diambil dari kasus dengan skor kemiripan tertinggi (0.2415).\n",
            "Prediksi solusi selesai.\n",
            "  Memproses Query ID: Q3_Perdata_Wanprestasi_1\n",
            "\n",
            "Melakukan retrieval untuk query: 'Gugatan wanprestasi karena salah satu pihak tidak ...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "\n",
            "Memprediksi solusi untuk query: 'Gugatan wanprestasi karena salah satu pihak tidak ...'\n",
            "\n",
            "Melakukan retrieval untuk query: 'Gugatan wanprestasi karena salah satu pihak tidak ...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "DEBUG: Solusi prediksi diambil dari kasus dengan skor kemiripan tertinggi (0.2513).\n",
            "Prediksi solusi selesai.\n",
            "  Memproses Query ID: Q4_Perdata_Perceraian_1\n",
            "\n",
            "Melakukan retrieval untuk query: 'Permohonan cerai gugat di Pengadilan Agama karena ...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "\n",
            "Memprediksi solusi untuk query: 'Permohonan cerai gugat di Pengadilan Agama karena ...'\n",
            "\n",
            "Melakukan retrieval untuk query: 'Permohonan cerai gugat di Pengadilan Agama karena ...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "DEBUG: Solusi prediksi diambil dari kasus dengan skor kemiripan tertinggi (0.4230).\n",
            "Prediksi solusi selesai.\n",
            "  Memproses Query ID: Q5_Pidana_Pencurian_1\n",
            "\n",
            "Melakukan retrieval untuk query: 'Kasus pencurian sepeda motor yang dilakukan oleh a...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "\n",
            "Memprediksi solusi untuk query: 'Kasus pencurian sepeda motor yang dilakukan oleh a...'\n",
            "\n",
            "Melakukan retrieval untuk query: 'Kasus pencurian sepeda motor yang dilakukan oleh a...' (top-k=5)\n",
            "Retrieval selesai. Ditemukan 5 kasus.\n",
            "DEBUG: Solusi prediksi diambil dari kasus dengan skor kemiripan tertinggi (0.4128).\n",
            "Prediksi solusi selesai.\n",
            "\n",
            "--- Memulai Evaluasi Retrieval (k=5) ---\n",
            "  Mengevaluasi Query ID: Q1_Narkotika_1\n",
            "  Mengevaluasi Query ID: Q2_Narkotika_2\n",
            "  Mengevaluasi Query ID: Q3_Perdata_Wanprestasi_1\n",
            "  Mengevaluasi Query ID: Q4_Perdata_Perceraian_1\n",
            "  Mengevaluasi Query ID: Q5_Pidana_Pencurian_1\n",
            "\n",
            "Evaluasi Retrieval Selesai.\n",
            "\n",
            "--- Ringkasan Metrik Retrieval ---\n",
            "Precision@5 Rata-rata: 0.0000\n",
            "Recall@5 Rata-rata:    0.0000\n",
            "F1-Score@5 Rata-rata:  0.0000\n",
            "\n",
            "Metrik retrieval berhasil disimpan ke: /content/drive/MyDrive/CBR_Data/data/eval/retrieval_metrics.csv\n",
            "\n",
            "--- Memulai Evaluasi Prediksi (Analisis Kualitatif/Sederhana) ---\n",
            "Implementasi ini memerlukan ground truth solusi yang terstruktur untuk setiap query agar metrik kuantitatif bisa dihitung.\n",
            "Untuk saat ini, akan membuat tabel hasil prediksi untuk analisis manual/kualitatif.\n",
            "Evaluasi Prediksi Selesai (Membuat Dataframe untuk Analisis Kualitatif).\n",
            "\n",
            "Hasil prediksi (untuk analisis kualitatif) berhasil disimpan ke: /content/drive/MyDrive/CBR_Data/data/eval/prediction_metrics.csv\n",
            "\n",
            "Untuk analisis kegagalan model (error analysis), periksa file prediction_metrics.csv secara manual.\n",
            "\n",
            "PROSES TAHAP 5 (MODEL EVALUATION) SELESAI.\n"
          ]
        }
      ]
    }
  ]
}